"""
Aplicaci√≥n de Streamlit para Predicci√≥n de Tendencias con TLOB
================================================================
Versi√≥n simplificada y robusta - Python 3.12
"""

import streamlit as st
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from pathlib import Path
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import sys

# Importar modelo
sys.path.append('.')
from src.models.tlob import TLOB

# Configuraci√≥n
st.set_page_config(
    page_title="TLOB - Predicci√≥n de Tendencias",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Constantes
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
EXAMPLES_DIR = Path("src/data/BTC/individual_examples")

# Checkpoints disponibles para diferentes horizontes
CHECKPOINTS = {
    10: "src/data/checkpoints/TLOB/BTC_seq_size_128_horizon_10_seed_42/pt/val_loss=0.624_epoch=2.pt",
    20: "src/data/checkpoints/TLOB/BTC_seq_size_128_horizon_20_seed_42/pt/val_loss=0.822_epoch=1.pt",
    50: "src/data/checkpoints/TLOB/BTC_seq_size_128_horizon_50_seed_42/pt/val_loss=0.962_epoch=0.pt",
    100: "src/data/checkpoints/TLOB/BTC_seq_size_128_horizon_100_seed_42/pt/val_loss=1.013_epoch=0.pt"
}

# Mapeo de clases (seg√∫n utils_data.py l√≠nea 158)
# percentage_change < -alpha ‚Üí label=2 (DOWN)
# percentage_change > alpha  ‚Üí label=0 (UP)
# else                      ‚Üí label=1 (STATIONARY)
CLASSES = {0: "UP üìà", 1: "STATIONARY ‚û°Ô∏è", 2: "DOWN üìâ"}
COLORS = {0: "#10b981", 1: "#3b82f6", 2: "#ef4444"}

# ============================================================================
# FUNCIONES
# ============================================================================

def get_model(horizon=10):
    """
    CARGA DE PESOS DEL MODELO
    =========================
    
    Esta funci√≥n carga el modelo TLOB pre-entrenado desde un checkpoint
    espec√≠fico seg√∫n el horizonte de predicci√≥n seleccionado.
    
    Args:
        horizon (int): Horizonte de predicci√≥n en timesteps (10, 20, 50, 100)
    
    Returns:
        TLOB: Modelo cargado con pesos pre-entrenados, listo para inferencia en modo eval()
    
    Proceso de Carga:
    -----------------
    1. Verificar si el modelo ya est√° en session_state (cach√©)
    2. Crear aliases de m√≥dulos antiguos (compatibilidad con checkpoints entrenados)
    3. Instanciar arquitectura TLOB con hiperpar√°metros correctos para BTC
    4. Cargar checkpoint .pt correspondiente al horizonte desde disco
    5. Limpiar keys del state_dict (remover prefijo 'model.' si existe)
    6. Cargar pesos en el modelo usando load_state_dict()
    7. Configurar modelo en modo evaluaci√≥n (.eval())
    8. Guardar en session_state para reutilizaci√≥n sin recarga
    
    Nota Importante - Aliases de M√≥dulos:
    -------------------------------------
    Los checkpoints fueron entrenados con la estructura antigua del repositorio
    (imports como 'config', 'models', etc. sin prefijo 'src.'). PyTorch serializa
    los imports usados durante el entrenamiento en el checkpoint.
    
    Para deserializar correctamente, necesitamos crear aliases en sys.modules:
        'config' ‚Üí 'src.config'
        'models' ‚Üí 'src.models'
        'utils' ‚Üí 'src.utils'
        etc.
    
    Esto permite a torch.load() encontrar los m√≥dulos correctos sin modificar
    los checkpoints entrenados originalmente.
    
    Checkpoints Disponibles:
    ------------------------
    - Horizonte 10: src/data/checkpoints/TLOB/BTC_seq_size_128_horizon_10_seed_42/pt/val_loss=0.624_epoch=2.pt
    - Horizonte 20: src/data/checkpoints/TLOB/BTC_seq_size_128_horizon_20_seed_42/pt/val_loss=0.822_epoch=1.pt
    - Horizonte 50: src/data/checkpoints/TLOB/BTC_seq_size_128_horizon_50_seed_42/pt/val_loss=0.962_epoch=0.pt
    - Horizonte 100: src/data/checkpoints/TLOB/BTC_seq_size_128_horizon_100_seed_42/pt/val_loss=1.013_epoch=0.pt
    
    Hiperpar√°metros del Modelo TLOB para BTC:
    ------------------------------------------
    - hidden_dim: 40 (dimensi√≥n de embeddings)
    - num_layers: 4 (n√∫mero de pares de TransformerLayers)
    - seq_size: 128 (longitud de secuencia temporal)
    - num_features: 40 (n√∫mero de features del LOB)
    - num_heads: 1 (cabezas de atenci√≥n por layer)
    - is_sin_emb: True (usar positional encoding sinusoidal)
    - dataset_type: "BTC"
    
    Ejemplo de Uso:
    ---------------
    ```python
    # Cargar modelo para horizonte de 10 timesteps
    model = get_model(horizon=10)
    
    # Primera llamada: carga desde disco (~2-3 segundos)
    # Llamadas subsecuentes: recupera desde session_state (instant√°neo)
    ```
    """
    # Crear una clave √∫nica para cada horizonte
    model_key = f'tlob_model_h{horizon}'
    
    if model_key not in st.session_state or st.session_state.get('current_horizon') != horizon:
        with st.spinner(f"üîÑ Cargando modelo TLOB (horizonte {horizon})..."):
            try:
                # IMPORTANTE: Crear alias para m√≥dulos antiguos en el checkpoint
                # El checkpoint fue entrenado con imports antiguos (config, models, etc.)
                # Necesitamos crear aliases para que PyTorch pueda deserializar
                import src.config
                import src.config.config
                import src.models
                import src.models.tlob
                import src.models.engine
                import src.utils
                import src.preprocessing
                import src.constants
                
                # Registrar aliases en sys.modules
                sys.modules['config'] = src.config
                sys.modules['config.config'] = src.config.config
                sys.modules['models'] = src.models
                sys.modules['models.tlob'] = src.models.tlob
                sys.modules['models.engine'] = src.models.engine
                sys.modules['utils'] = src.utils
                sys.modules['preprocessing'] = src.preprocessing
                sys.modules['constants'] = src.constants
                
                # Configuraci√≥n del modelo
                model = TLOB(
                    hidden_dim=40,
                    num_layers=4,
                    seq_size=128,
                    num_features=40,
                    num_heads=1,
                    is_sin_emb=True,
                    dataset_type="BTC"
                )
                model.to(DEVICE)
                model.eval()
                
                # Cargar pesos del checkpoint correspondiente al horizonte
                checkpoint_path = CHECKPOINTS[horizon]
                checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)
                state_dict = checkpoint["state_dict"]
                
                # Limpiar keys
                clean_dict = {}
                for k, v in state_dict.items():
                    clean_key = k.replace("model.", "") if k.startswith("model.") else k
                    clean_dict[clean_key] = v
                
                model.load_state_dict(clean_dict)
                st.session_state[model_key] = model
                st.session_state['current_horizon'] = horizon
                st.success(f"‚úÖ Modelo cargado (horizonte {horizon} timesteps)")
            except Exception as e:
                st.error(f"‚ùå Error cargando modelo: {e}")
                return None
    
    return st.session_state.get(model_key)

def get_examples():
    """Lista archivos de ejemplo"""
    if not EXAMPLES_DIR.exists():
        return []
    files = list(EXAMPLES_DIR.glob("example_*.npy"))
    return [f for f in files if not f.stem.endswith("_result")]

def calculate_alpha(data, horizon=10, use_spread=False, len_smooth=5):
    """
    Calcula el umbral alpha para clasificaci√≥n de tendencias
    
    Args:
        data: numpy array con datos LOB (shape: seq_len, num_features)
        horizon: horizonte de predicci√≥n
        use_spread: Si True, usa spread; si False, usa cambio porcentual
        len_smooth: longitud de ventana para suavizado
        
    Returns:
        alpha: umbral calculado
    """
    # Extraer precios ask (columna 0) y bid (columna 2)
    ask_prices = data[:, 0]
    bid_prices = data[:, 2]
    
    # Calcular mid-price
    mid_prices = (ask_prices + bid_prices) / 2
    
    if use_spread:
        # Alpha basado en spread promedio (como porcentaje del mid-price)
        spread = ask_prices - bid_prices
        avg_mid_price = mid_prices.mean()
        alpha = (spread.mean() / avg_mid_price) if avg_mid_price != 0 else 0.0
    else:
        # Alpha basado en cambio porcentual promedio
        # Simular el c√°lculo de labels para obtener alpha
        if horizon >= len(mid_prices):
            len_smooth = min(horizon, len_smooth)
        
        # Calcular cambio porcentual entre ventanas
        if len(mid_prices) > horizon + len_smooth:
            previous_prices = mid_prices[:-horizon]
            future_prices = mid_prices[horizon:]
            percentage_change = (future_prices - previous_prices) / previous_prices
            alpha = np.abs(percentage_change).mean() / 2
        else:
            # Si no hay suficientes datos, usar un alpha por defecto
            alpha = 0.002  # 0.2%
    
    return alpha

def normalize_raw_data(data):
    """
    PREPROCESAMIENTO: Z-SCORE NORMALIZATION
    ========================================
    
    Normaliza datos crudos del LOB usando Z-score para cada tipo de feature.
    Esta normalizaci√≥n es CR√çTICA porque el modelo TLOB fue entrenado con datos
    normalizados (mean‚âà0, std‚âà1).
    
    Args:
        data (np.array): Datos crudos shape (128, 40)
                        - Columnas pares (0, 2, 4, ..., 38): Precios (en USDT)
                        - Columnas impares (1, 3, 5, ..., 39): Vol√∫menes (en BTC)
    
    Returns:
        np.array: Datos normalizados (mean‚âà0, std‚âà1) shape (128, 40)
    
    Proceso de Normalizaci√≥n:
    -------------------------
    1. Convertir a DataFrame de pandas para manipulaci√≥n flexible
    2. Separar columnas por tipo:
       - Precios: columnas pares (0::2)
       - Vol√∫menes: columnas impares (1::2)
    3. Calcular estad√≠sticas globales por tipo:
       - mean_prices, std_prices: de TODAS las columnas pares
       - mean_volumes, std_volumes: de TODAS las columnas impares
    4. Aplicar Z-score a cada columna seg√∫n su tipo:
       - Precios: (x - mean_prices) / std_prices
       - Vol√∫menes: (x - mean_volumes) / std_volumes
    5. Retornar como numpy array
    
    Raz√≥n del Preprocesamiento:
    ---------------------------
    El modelo TLOB fue entrenado con datos normalizados. La normalizaci√≥n:
    - **Estabiliza el entrenamiento**: Evita gradientes explosivos
    - **Generalizaci√≥n**: Permite que el modelo funcione con diferentes rangos de precios
    - **Convergencia**: Facilita la optimizaci√≥n (gradientes m√°s estables)
    - **Comparabilidad**: Precios y vol√∫menes en escalas similares
    
    Ejemplo con Datos Reales de BTC:
    ---------------------------------
    Entrada (datos crudos):
    ```
    ASK_P1 = 42150.5 USDT, ASK_V1 = 0.524 BTC
    BID_P1 = 42148.2 USDT, BID_V1 = 0.631 BTC
    ...
    ```
    
    Despu√©s de normalizaci√≥n:
    ```
    ASK_P1 = 0.765, ASK_V1 = 0.909
    BID_P1 = -1.490, BID_V1 = -1.091
    ...
    ```
    
    Estad√≠sticas resultantes:
    ```
    mean_normalized ‚âà 0.0001 (casi 0)
    std_normalized ‚âà 0.998 (casi 1)
    ```
    
    Nota Importante:
    ----------------
    Esta funci√≥n normaliza GLOBALMENTE (usando estad√≠sticas de toda la ventana).
    Es diferente de la normalizaci√≥n por feature individual. El approach global
    es el usado durante el entrenamiento del modelo TLOB.
    """
    import pandas as pd
    
    df = pd.DataFrame(data)
    
    # Columnas pares = precios, impares = vol√∫menes
    mean_prices = df.iloc[:, 0::2].stack().mean()
    std_prices = df.iloc[:, 0::2].stack().std()
    mean_volumes = df.iloc[:, 1::2].stack().mean()
    std_volumes = df.iloc[:, 1::2].stack().std()
    
    # Normalizar
    for col in df.columns[0::2]:  # Precios
        df[col] = (df[col] - mean_prices) / std_prices
    
    for col in df.columns[1::2]:  # Vol√∫menes
        df[col] = (df[col] - mean_volumes) / std_volumes
    
    return df.values

def is_data_normalized(data):
    """
    Detecta si los datos ya est√°n normalizados
    
    Heur√≠stica: Si mean ‚âà 0 y std ‚âà 1, probablemente ya est√° normalizado
    Si mean >> 1000, probablemente son datos crudos (precios BTC)
    """
    mean = np.abs(data.mean())
    std = data.std()
    
    # Si el mean es muy grande, son datos crudos
    if mean > 100:
        return False, "raw"
    # Si mean ‚âà 0 y std ‚âà 1, ya est√° normalizado
    elif mean < 1 and 0.5 < std < 2:
        return True, "normalized"
    # No estamos seguros
    else:
        return None, "unknown"

def load_data(filepath):
    """
    Carga archivo .npy y normaliza autom√°ticamente si es necesario
    
    Returns:
        tuple: (data_normalized, data_raw) o (data_normalized, None) si ya est√° normalizado
    
    Soporta:
    - Archivos .npy ya normalizados
    - Archivos .npy crudos (se normalizan autom√°ticamente)
    - Archivos .csv crudos (se normalizan autom√°ticamente)
    """
    try:
        # Determinar tipo de archivo y extensi√≥n
        if hasattr(filepath, 'name'):  # UploadedFile de Streamlit
            file_extension = Path(filepath.name).suffix
            is_uploaded_file = True
        elif isinstance(filepath, str):
            filepath = Path(filepath)
            file_extension = filepath.suffix
            is_uploaded_file = False
        else:  # Path object
            file_extension = filepath.suffix
            is_uploaded_file = False
        
        # Cargar datos seg√∫n formato
        if file_extension == '.csv':
            import pandas as pd
            df = pd.read_csv(filepath)
            # Si tiene timestamp, eliminarlo
            if 'timestamp' in df.columns:
                df = df.drop(columns=['timestamp'])
            data = df.values
        elif file_extension == '.npy':
            data = np.load(filepath)
        else:
            st.error(f"‚ùå Formato no soportado: {file_extension}")
            return None, None
        
        # Verificar shape
        if data.shape != (128, 40):
            st.error(f"‚ùå Shape incorrecto: {data.shape}. Esperado: (128, 40)")
            return None, None
        
        # Detectar si necesita normalizaci√≥n
        is_normalized, data_type = is_data_normalized(data)
        
        if is_normalized == False:  # Datos crudos
            st.info("üîÑ Detectados datos crudos. Aplicando normalizaci√≥n Z-score...")
            data_raw = data.copy()  # Guardar copia de datos crudos
            data_normalized = normalize_raw_data(data)
            st.success(f"‚úÖ Normalizaci√≥n completada (mean={data_normalized.mean():.4f}, std={data_normalized.std():.4f})")
            return data_normalized, data_raw  # Retornar AMBOS
        elif is_normalized == True:  # Ya normalizado
            st.success(f"‚úÖ Datos ya normalizados (mean={data.mean():.4f}, std={data.std():.4f})")
            return data, None  # Solo datos normalizados, sin crudos
        else:  # No estamos seguros
            st.warning(f"‚ö†Ô∏è Tipo de datos ambiguo. Usando tal cual (mean={data.mean():.4f}, std={data.std():.4f})")
            return data, None
        
    except Exception as e:
        st.error(f"‚ùå Error: {e}")
        import traceback
        st.text(traceback.format_exc())
        return None, None

def run_prediction(model, data):
    """
    GENERACI√ìN DE INFERENCIA
    =========================
    
    Ejecuta la predicci√≥n del modelo TLOB sobre una ventana LOB.
    Esta funci√≥n maneja el forward pass y corrige el orden del softmax.
    
    Args:
        model (TLOB): Modelo cargado y en modo eval()
        data (np.array): Datos normalizados shape (128, 40)
    
    Returns:
        tuple: (logits, probs, pred)
            - logits (np.array): Salidas raw del modelo antes de softmax, shape (3,)
                                Orden: [UP, STATIONARY, DOWN]
            - probs (np.array): Probabilidades despu√©s de softmax, shape (3,)
                               Orden: [UP, STATIONARY, DOWN]
                               Suman a 1.0
            - pred (int): Clase predicha (0=UP, 1=STATIONARY, 2=DOWN)
                         Resultado de argmax(probs)
    
    Proceso de Inferencia:
    ----------------------
    1. **Preparar input**:
       - Convertir numpy array a torch tensor
       - Agregar dimensi√≥n de batch: (128,40) ‚Üí (1,128,40)
       - Mover a device (CPU o GPU)
    
    2. **Forward pass sin gradientes**:
       - Usar torch.no_grad() para ahorrar memoria
       - Ejecutar model(x) para obtener logits raw
       - Shape de salida: (1, 3) ‚Üí extraer [0] para obtener (3,)
    
    3. **Aplicar softmax**:
       - Convertir logits a probabilidades
       - softmax(x_i) = exp(x_i) / sum(exp(x_j))
       - Resultado: 3 valores que suman 1.0
    
    4. **‚ö†Ô∏è INVERSI√ìN CR√çTICA DEL ORDEN**:
       - El modelo da orden inverso a las etiquetas
       - Reordenar para coincidir con CLASSES mapping
    
    5. **Obtener predicci√≥n final**:
       - argmax(probs) ‚Üí clase con mayor probabilidad
    
    ‚ö†Ô∏è IMPORTANTE: INVERSI√ìN DEL ORDEN DEL SOFTMAX
    -----------------------------------------------
    
    ### El Problema:
    
    Durante el entrenamiento, las etiquetas se asignaron as√≠ (utils_data.py l√≠nea 158):
    ```python
    labels = np.where(
        percentage_change < -alpha, 2,  # DOWN
        np.where(percentage_change > alpha, 0, 1)  # UP, STATIONARY
    )
    ```
    
    Por lo tanto:
    - **Etiqueta 0** = UP üìà (cambio > +alpha)
    - **Etiqueta 1** = STATIONARY ‚û°Ô∏è (cambio dentro de ¬±alpha)
    - **Etiqueta 2** = DOWN üìâ (cambio < -alpha)
    
    ### El Modelo (PyTorch):
    
    Sin embargo, el modelo de PyTorch aprende a dar salidas en orden NUM√âRICO
    de las etiquetas durante el entrenamiento, resultando en:
    
    ```
    softmax_raw[0] = probabilidad de etiqueta 2 (DOWN)
    softmax_raw[1] = probabilidad de etiqueta 1 (STATIONARY)
    softmax_raw[2] = probabilidad de etiqueta 0 (UP)
    ```
    
    Esto es [DOWN, STATIONARY, UP] en lugar de [UP, STATIONARY, DOWN]
    
    ### La Soluci√≥n:
    
    Invertimos el orden para que coincida con el mapeo de etiquetas:
    ```python
    logits = [logits_raw[2], logits_raw[1], logits_raw[0]]
    probs = [probs_raw[2], probs_raw[1], probs_raw[0]]
    ```
    
    Ahora:
    - **probs[0]** = probabilidad de UP (etiqueta 0) ‚úì
    - **probs[1]** = probabilidad de STATIONARY (etiqueta 1) ‚úì
    - **probs[2]** = probabilidad de DOWN (etiqueta 2) ‚úì
    
    Esto asegura que `CLASSES[pred]` retorne la etiqueta correcta.
    
    Ejemplo de Inferencia:
    ----------------------
    ```python
    # Input: ventana LOB normalizada (128, 40)
    logits, probs, pred = run_prediction(model, data)
    
    # Output ejemplo:
    # logits = [2.341, 0.156, -1.892]  # [UP, STATIONARY, DOWN]
    # probs = [0.852, 0.123, 0.025]     # [85.2%, 12.3%, 2.5%]
    # pred = 0                           # Clase UP
    # CLASSES[pred] = "UP üìà"
    ```
    
    Verificaci√≥n:
    -------------
    Para verificar que el orden es correcto, se puede comparar con predicciones
    de ejemplos conocidos del conjunto de validaci√≥n.
    """
    try:
        x = torch.from_numpy(data[None, :, :]).float().to(DEVICE)
        with torch.no_grad():
            logits_raw = model(x)[0].cpu().numpy()
            probs_raw = F.softmax(torch.from_numpy(logits_raw), dim=0).numpy()
            
            # INVERTIR orden para que coincida con etiquetas
            # probs_raw = [DOWN, STABLE, UP]
            # probs = [UP, STABLE, DOWN] (orden de etiquetas)
            logits = np.array([logits_raw[2], logits_raw[1], logits_raw[0]])
            probs = np.array([probs_raw[2], probs_raw[1], probs_raw[0]])
            
            pred = int(np.argmax(probs))
        return logits, probs, pred
    except Exception as e:
        st.error(f"‚ùå Error en predicci√≥n: {e}")
        return None, None, None

def plot_heatmap(data):
    """Heatmap de la ventana LOB"""
    fig = go.Figure(data=go.Heatmap(
        z=data.T,
        x=list(range(128)),
        y=list(range(40)),
        colorscale='RdYlBu_r',
        colorbar=dict(title="Z-score")
    ))
    fig.update_layout(
        title="Heatmap LOB (128 √ó 40)",
        xaxis_title="Timestep",
        yaxis_title="Feature",
        height=500
    )
    return fig

def plot_timeseries(data):
    """Series temporales de features clave"""
    fig = go.Figure()
    features = [(0, "ASK Price", "#ef4444"), (10, "ASK Vol", "#f97316"),
                (20, "BID Price", "#10b981"), (30, "BID Vol", "#3b82f6")]
    
    for idx, name, color in features:
        fig.add_trace(go.Scatter(
            x=list(range(128)),
            y=data[:, idx],
            mode='lines',
            name=name,
            line=dict(color=color, width=2)
        ))
    
    fig.update_layout(
        title="Evoluci√≥n Temporal",
        xaxis_title="Timestep",
        yaxis_title="Valor",
        height=400,
        hovermode='x unified'
    )
    return fig

def plot_distributions(data):
    """Distribuciones de las 40 features"""
    # 8 filas x 5 columnas = 40 features
    fig, axes = plt.subplots(8, 5, figsize=(20, 24))
    fig.suptitle("Distribuci√≥n de las 40 Features del LOB", fontsize=16, fontweight='bold')
    
    for i in range(40):
        row = i // 5
        col = i % 5
        ax = axes[row, col]
        ax.hist(data[:, i], bins=15, alpha=0.7, edgecolor='black', color='steelblue')
        
        # Nombres m√°s descriptivos
        if i < 10:
            label = f'F{i}: ASK Price L{i+1}'
        elif i < 20:
            label = f'F{i}: ASK Vol L{i-9}'
        elif i < 30:
            label = f'F{i}: BID Price L{i-19}'
        else:
            label = f'F{i}: BID Vol L{i-29}'
        
        ax.set_title(label, fontsize=9)
        ax.set_xlabel('Valor', fontsize=8)
        ax.set_ylabel('Frecuencia', fontsize=8)
        ax.tick_params(labelsize=7)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def plot_probs_bar(probs):
    """Gr√°fico de barras de probabilidades"""
    fig = go.Figure(data=[
        go.Bar(
            x=list(CLASSES.values()),
            y=probs * 100,
            marker_color=list(COLORS.values()),
            text=[f"{p:.1%}" for p in probs],
            textposition='auto'
        )
    ])
    fig.update_layout(
        title="Probabilidades",
        yaxis_title="Probabilidad (%)",
        height=400,
        showlegend=False
    )
    return fig

# ============================================================================
# APP PRINCIPAL
# ============================================================================

def main():
    """
    APLICACI√ìN STREAMLIT - INTERFAZ PRINCIPAL
    ==========================================
    
    Interfaz web interactiva para inferencia con TLOB.
    Permite cargar datos LOB, visualizarlos, configurar par√°metros y ejecutar predicciones.
    
    Estructura de la Aplicaci√≥n:
    ----------------------------
    
    ### 1. CONFIGURACI√ìN INICIAL (l√≠neas 567-587)
    ```python
    st.set_page_config(
        page_title="TLOB - Predicci√≥n de Tendencias",
        page_icon="üìà",
        layout="wide",  # Usa todo el ancho de la pantalla
        initial_sidebar_state="expanded"
    )
    ```
    
    Configura:
    - T√≠tulo de la pesta√±a del navegador
    - Icono (emoji)
    - Layout ancho para mejor visualizaci√≥n
    - Sidebar expandido por defecto
    
    ### 2. SIDEBAR - CARGA DE DATOS (l√≠neas 588-662)
    
    **Selector de Fuente:**
    - üì¶ Preprocesados: Archivos .npy ya normalizados (mean‚âà0, std‚âà1)
    - üìÑ Crudos: Archivos .csv o .npy sin normalizar
    
    **Flujo de Carga:**
    1. Usuario selecciona fuente (radio buttons)
    2. Se buscan archivos en el directorio correspondiente
    3. Usuario selecciona un archivo (selectbox)
    4. Click en "Cargar" ejecuta load_data()
    5. Datos se guardan en st.session_state['data']
    6. Datos raw (si existen) se guardan en st.session_state['data_raw']
    
    **File Uploader:**
    - Permite subir archivos personalizados
    - Soporta .npy y .csv
    - Detecci√≥n autom√°tica de normalizaci√≥n
    
    ### 3. EXPANDER CON INFORMACI√ìN (l√≠neas 588-600)
    
    Explica el orden del etiquetado y softmax:
    - C√≥mo se asignaron las etiquetas durante entrenamiento
    - Por qu√© el modelo da orden inverso
    - C√≥mo la app corrige autom√°ticamente
    
    ### 4. TABS DE LA INTERFAZ (l√≠neas 740-1089)
    
    #### **TAB 1 - üìä Datos** (l√≠neas 745-814)
    
    **Visualizaciones:**
    - M√©tricas: Shape, Mean, Std, Range
    - Heatmap interactivo (plotly): 128√ó40 matriz de valores
    - Series temporales: Evoluci√≥n de ASK/BID prices y volumes
    - Tabla completa de datos (expandible)
    
    **Comparaci√≥n Raw vs Normalized:**
    Si se cargaron datos crudos:
    - Lado izquierdo: Datos originales (precios en USDT, vol√∫menes en BTC)
    - Lado derecho: Datos normalizados (z-scores)
    - Muestra primeras 10 filas de cada tipo
    
    **C√≥digo Clave:**
    ```python
    data = st.session_state['data']  # Datos normalizados
    data_raw = st.session_state.get('data_raw', None)  # Datos crudos (opcional)
    
    if data_raw is not None:
        # Mostrar comparaci√≥n lado a lado
        col_raw, col_norm = st.columns(2)
        with col_raw:
            st.metric("Mean", f"{data_raw.mean():.2f}")
        with col_norm:
            st.metric("Mean", f"{data.mean():.6f}")
    ```
    
    #### **TAB 2 - üîç An√°lisis** (l√≠neas 817-855)
    
    **Distribuciones:**
    - 40 histogramas (8 filas √ó 5 columnas)
    - Cada feature tiene su propio histograma
    - Nombres descriptivos: "F0: ASK Price L1", "F10: ASK Vol L1", etc.
    
    **Estad√≠sticas Descriptivas:**
    - Tabla con Mean, Std, Min, Max para cada feature
    - Formato num√©rico consistente (3 decimales)
    - 600px de altura para scroll c√≥modo
    
    **C√≥digo Clave:**
    ```python
    stats = []
    for i in range(40):
        feat = data[:, i]
        if i < 10:
            label = f'F{i}: ASK Price L{i+1}'
        elif i < 20:
            label = f'F{i}: ASK Vol L{i-9}'
        # ... etc
        stats.append({'Feature': label, 'Mean': feat.mean(), ...})
    ```
    
    #### **TAB 3 - üéØ Predicci√≥n** (l√≠neas 858-977)
    
    **Selectores de Configuraci√≥n:**
    
    1. **Horizonte de Predicci√≥n** (selectbox):
       - Opciones: 10, 20, 50, 100 timesteps
       - Cada horizonte usa un modelo diferente
       - Horizonte 10 ‚âà 0.5 segundos
       - Horizonte 100 ‚âà 5 segundos
    
    2. **Tipo de Umbral (Alpha)** (radio buttons):
       - üìä Normal: alpha = mean(|% change|) / 2
         * Basado en volatilidad natural
         * Usado durante entrenamiento
       - üíπ Spread: alpha = mean(ask - bid) / mid_price
         * Basado en costos de transacci√≥n
         * M√°s restrictivo (solo cambios > spread son rentables)
    
    **Info Box Explicativo:**
    - Explica c√≥mo se etiquetan las tendencias
    - Muestra configuraci√≥n actual (horizonte, umbral)
    - Nota sobre inversi√≥n autom√°tica del softmax
    
    **Bot√≥n "Ejecutar Predicci√≥n":**
    1. Verifica que hay datos cargados
    2. Carga el modelo para el horizonte seleccionado
    3. Calcula alpha (din√°mico si hay datos raw, te√≥rico si no)
    4. Ejecuta run_prediction()
    5. Guarda resultados en session_state['pred_result']
    6. Muestra balloons üéà y recarga la app
    
    **C√≥digo Clave:**
    ```python
    if st.button("üöÄ Ejecutar Predicci√≥n", type="primary"):
        model = get_model(horizon=horizon)
        
        # Calcular alpha
        data_for_alpha = st.session_state.get('data_raw', None)
        if data_for_alpha is not None:
            alpha = calculate_alpha(data_for_alpha, horizon, use_spread)
        else:
            alpha = 0.005 if use_spread else 0.002  # Te√≥rico
        
        # Predicci√≥n
        logits, probs, pred = run_prediction(model, data)
        
        # Guardar resultados
        st.session_state['pred_result'] = {
            'logits': logits,
            'probs': probs,
            'pred': pred
        }
        st.rerun()
    ```
    
    #### **TAB 4 - üìà Resultados** (l√≠neas 980-1089)
    
    **Resultado Principal:**
    - Visualizaci√≥n grande centrada con emoji
    - Color de fondo seg√∫n predicci√≥n:
      * Verde: UP üìà
      * Azul: STATIONARY ‚û°Ô∏è
      * Rojo: DOWN üìâ
    - Confianza en porcentaje
    
    **Info Box de Configuraci√≥n:**
    - Horizonte usado
    - Tipo de umbral
    - Alpha calculado o te√≥rico
    - Interpretaci√≥n del alpha
    
    **M√©tricas de Probabilidades:**
    - 3 columnas con st.metric()
    - UP, STATIONARY, DOWN
    - Muestra probabilidad y logit
    
    **Gr√°fico de Barras:**
    - Plotly bar chart interactivo
    - Colores seg√∫n clase (verde/azul/rojo)
    - Valores en porcentaje
    
    **Interpretaci√≥n Autom√°tica:**
    - Nivel de confianza:
      * >90%: MUY ALTA ‚≠ê‚≠ê‚≠ê
      * >75%: ALTA ‚≠ê‚≠ê
      * >60%: MODERADA ‚≠ê
      * <60%: BAJA
    - Explicaci√≥n textual de la predicci√≥n
    
    **Expander "Detalles T√©cnicos":**
    - Shape de entrada
    - Mean y Std
    - Logits raw
    - Probabilidades post-softmax
    - Predicci√≥n final
    
    ### 5. GESTI√ìN DE ESTADO CON SESSION STATE
    
    **Variables Clave:**
    ```python
    st.session_state = {
        'data': np.array,              # Datos normalizados (128, 40)
        'data_raw': np.array,          # Datos crudos (128, 40) [opcional]
        'filename': str,               # Nombre del archivo cargado
        'source': str,                 # Fuente: "Preprocesados" o "Crudos"
        
        'tlob_model_h10': TLOB,        # Modelo para horizonte 10
        'tlob_model_h20': TLOB,        # Modelo para horizonte 20
        'tlob_model_h50': TLOB,        # Modelo para horizonte 50
        'tlob_model_h100': TLOB,       # Modelo para horizonte 100
        'current_horizon': int,        # Horizonte actual
        
        'pred_result': dict,           # Resultados de predicci√≥n
            # {'logits': array, 'probs': array, 'pred': int}
        'horizon': int,                # Horizonte usado en predicci√≥n
        'use_spread': bool,            # Tipo de umbral usado
        'alpha': float,                # Alpha calculado
        'alpha_type': str,             # "Normal" o "Spread"
        'alpha_calculated': bool,      # True si din√°mico, False si te√≥rico
    }
    ```
    
    **Ventajas de Session State:**
    - No recargar modelo en cada interacci√≥n
    - Mantener datos cargados entre tabs
    - Preservar resultados de predicciones
    - Experiencia de usuario fluida sin p√©rdida de estado
    
    ### 6. FLUJO DE USUARIO T√çPICO
    
    ```
    1. Usuario abre app en navegador (http://localhost:8501)
    2. Sidebar: Selecciona fuente de datos (Preprocesados o Crudos)
    3. Sidebar: Selecciona archivo y click "Cargar"
    4. load_data() detecta si necesita normalizaci√≥n
    5. Datos se guardan en session_state
    6. TAB 1: Usuario visualiza datos (heatmap, series temporales)
    7. TAB 2: Usuario explora distribuciones y estad√≠sticas
    8. TAB 3: Usuario configura horizonte y umbral
    9. TAB 3: Click en "Ejecutar Predicci√≥n"
       a. get_model(horizon) carga modelo apropiado (o usa cach√©)
       b. calculate_alpha() determina umbral
       c. run_prediction() genera inferencia
       d. Resultados se guardan en session_state
   10. TAB 4: Usuario ve predicci√≥n final con visualizaciones
   11. Usuario puede cargar otro ejemplo o probar diferentes configuraciones
    ```
    
    ### 7. BOT√ìN "NUEVO EJEMPLO"
    
    Ubicado en la parte superior (l√≠nea 736):
    - Limpia todo el estado excepto modelos cargados
    - Permite empezar de cero sin recargar la app
    - Mantiene modelos en memoria para rapidez
    
    **C√≥digo:**
    ```python
    if st.button("üîÑ Nuevo Ejemplo"):
        for key in list(st.session_state.keys()):
            if not key.startswith('tlob_model'):  # Mantener modelos
                del st.session_state[key]
        st.rerun()
    ```
    
    ### 8. MANEJO DE ERRORES Y VALIDACIONES
    
    **Verificaciones de Estado:**
    - Si no hay datos: Mostrar info y m√©tricas de archivos disponibles
    - Si no hay predicci√≥n: Mostrar warning en TAB 4
    - Si falla carga de modelo: Mostrar error con traceback
    
    **Validaciones de Datos:**
    - Shape correcto (128, 40)
    - Formato soportado (.npy, .csv)
    - Detecci√≥n de normalizaci√≥n (evitar doble normalizaci√≥n)
    
    ### 9. OPTIMIZACIONES DE PERFORMANCE
    
    **Cach√© de Modelos:**
    - Modelos se cargan una sola vez por horizonte
    - Se guardan en session_state con clave √∫nica
    - No se recargan en cada predicci√≥n
    
    **Lazy Loading:**
    - Modelo se carga solo cuando se ejecuta predicci√≥n
    - No se cargan todos los modelos al inicio
    
    **Session State Persistence:**
    - Datos persisten entre tabs
    - No hay recarga de archivos al cambiar de tab
    
    ### 10. INTEGRACI√ìN VISUAL
    
    **Plotly (Gr√°ficos Interactivos):**
    - Heatmap: Hover muestra valores exactos
    - Series temporales: Zoom, pan, hover
    - Barras de probabilidades: Interactivo
    
    **Matplotlib (Distribuciones):**
    - 40 histogramas en grid 8√ó5
    - Exportable como imagen
    
    **Streamlit Components:**
    - Metrics: N√∫meros grandes con deltas
    - Expanders: Info adicional colapsable
    - Columns: Layout responsive
    - Spinner: Feedback visual durante carga
    """
    # Header
    st.title("üìà TLOB: Predicci√≥n de Tendencias de Precios")
    st.markdown("""
    **Modelo:** Transformer con Dual Attention  
    **Dataset:** Bitcoin LOB (Enero 2023)
    
    Predice tendencias de precios (UP/DOWN/STATIONARY) usando datos de Limit Order Book.
    """)
    st.divider()
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configuraci√≥n")
        st.info(f"""
        **Arquitectura:** TLOB  
        **Par√°metros:** 1.1M  
        **Device:** {DEVICE}
        """)
        
        with st.expander("‚ÑπÔ∏è Sobre el etiquetado y salida del modelo"):
            st.markdown("""
            **Etiquetado durante el entrenamiento:**
            
            En `utils_data.py` l√≠nea 158:
            ```python
            labels = np.where(
                percentage_change < -alpha, 2,  # DOWN
                np.where(percentage_change > alpha, 0, 1)  # UP, STATIONARY
            )
            ```
            
            **Etiquetas (ground truth):**
            - **Clase 0**: UP üìà (cambio > +alpha)
            - **Clase 1**: STATIONARY ‚û°Ô∏è (cambio dentro de ¬±alpha)
            - **Clase 2**: DOWN üìâ (cambio < -alpha)
            
            **‚ö†Ô∏è IMPORTANTE: Orden del softmax**
            
            El modelo de PyTorch da salidas en **ORDEN INVERSO**:
            ```
            softmax[0] = probabilidad de DOWN (etiqueta 2)
            softmax[1] = probabilidad de STATIONARY (etiqueta 1)
            softmax[2] = probabilidad de UP (etiqueta 0)
            ```
            
            La app **invierte autom√°ticamente** las probabilidades para mostrarlas correctamente.
            """)
        
        st.divider()
        
        # ============ CARGAR DATOS ============
        st.subheader("üìÇ Cargar Datos")
        
        # Selector de fuente
        example_source = st.radio(
            "Fuente:",
            ["üì¶ Preprocesados", "üìÑ Crudos (CSV/NPY)"],
            help="Preprocesados: Ya normalizados. Crudos: Se normalizan autom√°ticamente"
        )
        
        # Cargar seg√∫n fuente
        if example_source == "üì¶ Preprocesados":
            examples_dir = Path("src/data/BTC/individual_examples")
            examples = sorted(examples_dir.glob("example_*.npy"))
            source_key = "prep"
        else:
            examples_dir = Path("src/data/BTC/raw_examples")
            # Buscar archivos CSV crudos, NPY crudos y NPY normalizados
            csv_examples = sorted(examples_dir.glob("raw_example_*.csv"))
            npy_raw_examples = sorted(examples_dir.glob("raw_example_*.npy"))
            npy_norm_examples = sorted(examples_dir.glob("normalized_example_*.npy"))
            examples = csv_examples + npy_raw_examples + npy_norm_examples
            source_key = "raw"
        
        if examples:
            st.markdown(f"**{len(examples)} ejemplos:**")
            example_names = [f.name for f in examples]
            selected_name = st.selectbox(
                "Selecciona:",
                example_names,
                key=f"example_selector_{source_key}"
            )
            
            if st.button("üîÑ Cargar", type="primary", key=f"load_btn_{source_key}"):
                selected_file = None
                for f in examples:
                    if f.name == selected_name:
                        selected_file = f
                        break
                
                if selected_file:
                    data_normalized, data_raw = load_data(selected_file)
                    if data_normalized is not None:
                        st.session_state['data'] = data_normalized
                        st.session_state['data_raw'] = data_raw  # Guardar datos crudos tambi√©n
                        st.session_state['filename'] = selected_name
                        st.session_state['source'] = example_source
                        if 'pred_result' in st.session_state:
                            del st.session_state['pred_result']
                        st.success(f"‚úÖ {selected_name}")
                        st.rerun()
        else:
            st.warning(f"‚ö†Ô∏è No hay ejemplos en {examples_dir}")
            if source_key == "raw":
                st.info("üí° Ejecuta:\n`python3 create_raw_examples.py`")
        
        st.divider()
        
        # Upload personalizado
        st.markdown("**O sube archivo:**")
        uploaded = st.file_uploader("Archivo .npy o .csv", type=['npy', 'csv'], key='file_uploader')
        
        if uploaded is not None:
            # Verificar si es un archivo nuevo
            current_filename = uploaded.name
            previous_filename = st.session_state.get('filename', None)
            
            # Si es un archivo diferente, limpiar estado y cargar nuevo
            if current_filename != previous_filename:
                # Limpiar resultados anteriores
                for key in ['prediction', 'probabilities', 'logits']:
                    if key in st.session_state:
                        del st.session_state[key]
                
                # Cargar nuevo archivo
                data_normalized, data_raw = load_data(uploaded)
                if data_normalized is not None:
                    st.session_state['data'] = data_normalized
                    st.session_state['data_raw'] = data_raw
                    st.session_state['filename'] = current_filename
                    st.session_state['source'] = "üìÅ Subido"
                    st.success(f"‚úÖ Archivo cargado: {current_filename}")
                    st.rerun()  # Forzar recarga de la interfaz
    
    # Main content
    if 'data' not in st.session_state:
        st.info("üëà Selecciona un ejemplo o sube un archivo .npy")
        
        # Contar ejemplos de ambas fuentes
        prep_examples = len(list(Path("src/data/BTC/individual_examples").glob("example_*.npy")))
        raw_csv_examples = len(list(Path("src/data/BTC/raw_examples").glob("raw_example_*.csv")))
        raw_npy_examples = len(list(Path("src/data/BTC/raw_examples").glob("raw_example_*.npy")))
        norm_npy_examples = len(list(Path("src/data/BTC/raw_examples").glob("normalized_example_*.npy")))
        
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("üì¶ Preprocesados", prep_examples)
        col2.metric("üìÑ CSV/NPY Crudos", raw_csv_examples + raw_npy_examples)
        col3.metric("‚úÖ Normalizados", norm_npy_examples)
        col4.metric("Clases", "3")
        
        with st.expander("‚ÑπÔ∏è ¬øQu√© es el LOB?"):
            st.markdown("""
            El **Limit Order Book** registra √≥rdenes de compra/venta pendientes.
            
            **40 features:**
            - 0-9: ASK Prices (10 niveles)
            - 10-19: ASK Volumes
            - 20-29: BID Prices
            - 30-39: BID Volumes
            
            **128 timesteps consecutivos**
            """)
        
        with st.expander("üß† ¬øC√≥mo funciona TLOB?"):
            st.markdown("""
            **Dual Attention:**
            - **Spatial:** Relaciones entre features
            - **Temporal:** Evoluci√≥n temporal
            
            **Output:** DOWN / STATIONARY / UP
            """)
        return
    
    # Datos cargados
    data = st.session_state['data']
    filename = st.session_state.get('filename', 'archivo')
    source = st.session_state.get('source', 'Desconocido')
    
    # Header con bot√≥n para limpiar
    col_info, col_clear = st.columns([4, 1])
    with col_info:
        st.success(f"‚úÖ **Archivo:** {filename}  |  **Fuente:** {source}")
    with col_clear:
        if st.button("üîÑ Nuevo Ejemplo", use_container_width=True):
            # Limpiar todo el estado
            for key in list(st.session_state.keys()):
                if key != 'tlob_model':  # Mantener el modelo cargado
                    del st.session_state[key]
            st.rerun()
    
    # Tabs
    tab1, tab2, tab3, tab4 = st.tabs(["üìä Datos", "üîç An√°lisis", "üéØ Predicci√≥n", "üìà Resultados"])
    
    # TAB 1: Datos
    with tab1:
        st.header("üìä Visualizaci√≥n")
        
        # Mostrar comparaci√≥n si hay datos crudos disponibles
        data_raw = st.session_state.get('data_raw', None)
        if data_raw is not None:
            st.info("üîÑ **Preprocesamiento Aplicado**: Este archivo fue cargado con datos crudos y normalizado autom√°ticamente")
            
            # Comparaci√≥n lado a lado
            col_raw, col_norm = st.columns(2)
            
            with col_raw:
                st.markdown("### üì• Datos Originales (Crudos)")
                st.caption("Valores reales del mercado BTC")
                st.metric("Mean", f"{data_raw.mean():.2f}", help="Promedio de precios y vol√∫menes sin normalizar")
                st.metric("Std", f"{data_raw.std():.2f}", help="Desviaci√≥n est√°ndar sin normalizar")
                st.metric("Range", f"{data_raw.min():.1f} ~ {data_raw.max():.1f}", help="Rango de valores")
                
                # Mostrar primeras filas de datos crudos
                with st.expander("üî¢ Ver primeras 10 filas"):
                    df_raw = pd.DataFrame(
                        data_raw[:10, :10],  # Primeras 10 filas, primeras 10 features
                        columns=[f"F{i}" for i in range(10)],
                        index=[f"T{i}" for i in range(10)]
                    )
                    st.dataframe(df_raw.style.format("{:.2f}"), height=400)
                    st.caption("Precios en USDT, vol√∫menes en BTC")
            
            with col_norm:
                st.markdown("### ‚úÖ Datos Normalizados")
                st.caption("Z-score: mean‚âà0, std‚âà1")
                st.metric("Mean", f"{data.mean():.6f}", help="Promedio despu√©s de normalizaci√≥n")
                st.metric("Std", f"{data.std():.6f}", help="Desviaci√≥n est√°ndar despu√©s de normalizaci√≥n")
                st.metric("Range", f"{data.min():.2f} ~ {data.max():.2f}", help="Rango de z-scores")
                
                # Mostrar primeras filas de datos normalizados
                with st.expander("üî¢ Ver primeras 10 filas"):
                    df_norm = pd.DataFrame(
                        data[:10, :10],  # Primeras 10 filas, primeras 10 features
                        columns=[f"F{i}" for i in range(10)],
                        index=[f"T{i}" for i in range(10)]
                    )
                    st.dataframe(df_norm.style.format("{:.6f}"), height=400)
                    st.caption("Z-scores normalizados")
            
            st.divider()
        
        # M√©tricas generales
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("Shape", f"{data.shape[0]} √ó {data.shape[1]}")
        c2.metric("Mean", f"{data.mean():.3f}")
        c3.metric("Std", f"{data.std():.3f}")
        c4.metric("Range", f"{data.min():.1f} ~ {data.max():.1f}")
        
        st.divider()
        
        st.subheader("üå°Ô∏è Heatmap")
        st.plotly_chart(plot_heatmap(data), use_container_width=True)
        
        st.subheader("üìà Series Temporales")
        st.plotly_chart(plot_timeseries(data), use_container_width=True)
        
        with st.expander("üî¢ Datos Num√©ricos Completos (128√ó40)"):
            # Mostrar TODOS los 128 timesteps y las 40 features
            df = pd.DataFrame(
                data[:, :40],  # Todos los timesteps, todas las features
                columns=[f"F{i}" for i in range(40)],
                index=[f"T{i}" for i in range(128)]
            )
            st.caption("üìå Matriz completa: 128 timesteps √ó 40 features")
            st.dataframe(df.style.format("{:.3f}"), height=600)
    
    # TAB 2: An√°lisis
    with tab2:
        st.header("üîç An√°lisis Estad√≠stico")
        
        st.subheader("üìä Distribuciones")
        st.pyplot(plot_distributions(data))
        
        st.subheader("üìà Estad√≠sticas de las 40 Features")
        stats = []
        # Ahora mostramos todas las 40 features
        for i in range(40):
            feat = data[:, i]
            
            # Nombres descriptivos para cada feature
            if i < 10:
                label = f'F{i}: ASK Price L{i+1}'
            elif i < 20:
                label = f'F{i}: ASK Vol L{i-9}'
            elif i < 30:
                label = f'F{i}: BID Price L{i-19}'
            else:
                label = f'F{i}: BID Vol L{i-29}'
            
            stats.append({
                'Feature': label,
                'Mean': feat.mean(),
                'Std': feat.std(),
                'Min': feat.min(),
                'Max': feat.max()
            })
        
        # Formatear solo columnas num√©ricas
        stats_df = pd.DataFrame(stats)
        st.dataframe(stats_df.style.format({
            'Mean': '{:.3f}',
            'Std': '{:.3f}',
            'Min': '{:.3f}',
            'Max': '{:.3f}'
        }), height=600)
    
    # TAB 3: Predicci√≥n
    with tab3:
        st.header("üéØ Realizar Predicci√≥n")
        
        # ============ CONFIGURACI√ìN DE PREDICCI√ìN ============
        st.subheader("‚öôÔ∏è Par√°metros de Predicci√≥n")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Selector de horizonte
            horizon = st.selectbox(
                "**Horizonte de Predicci√≥n:**",
                options=[10, 20, 50, 100],
                index=0,
                help="""
                N√∫mero de timesteps hacia el futuro para predecir:
                - 10: ~0.5 segundos
                - 20: ~1 segundo  
                - 50: ~2.5 segundos
                - 100: ~5 segundos
                """
            )
        
        with col2:
            # Selector de tipo de umbral
            threshold_type = st.radio(
                "**Tipo de Umbral (Alpha):**",
                options=["üìä Normal", "üíπ Spread"],
                index=0,
                help="""
                **Normal:** alpha = mean(|% change|) / 2
                - Basado en la volatilidad natural del activo
                - Usado durante el entrenamiento del modelo
                
                **Spread:** alpha = mean(ask - bid) / mid_price  
                - Basado en costos de transacci√≥n reales
                - M√°s restrictivo: solo cambios > spread son rentables
                - √ötil para evaluar estrategias de trading reales
                
                ‚ö†Ô∏è NOTA: El modelo fue entrenado con umbral Normal.
                Cambiar a Spread es solo para an√°lisis de rentabilidad.
                """
            )
        
        use_spread = (threshold_type == "üíπ Spread")
        threshold_name = "Spread" if use_spread else "Normal"
        
        st.divider()
        
        st.info(f"""
        El modelo predice la **tendencia** en los pr√≥ximos **{horizon} timesteps**:
        
        **Etiquetado durante entrenamiento** (utils_data.py):
        - Si `cambio_porcentual > +alpha` ‚Üí **UP üìà** (clase 0)
        - Si `cambio_porcentual < -alpha` ‚Üí **DOWN üìâ** (clase 2)
        - Si est√° dentro de ¬±alpha ‚Üí **STATIONARY ‚û°Ô∏è** (clase 1)
        
        **Nota:** El modelo da softmax en orden inverso [DOWN, STABLE, UP], 
        pero la app lo invierte autom√°ticamente para mostrar correctamente.
        
        **Configuraci√≥n actual:**
        - Horizonte: {horizon} timesteps
        - Umbral: {threshold_name}
        """)
        
        if st.button("üöÄ Ejecutar Predicci√≥n", type="primary", use_container_width=True):
            # Verificar que hay datos cargados
            if 'data' not in st.session_state:
                st.error("‚ö†Ô∏è Primero debes cargar datos en la pesta√±a 'Datos'")
                st.stop()
            
            data = st.session_state['data']
            
            model = get_model(horizon=horizon)
            if model is not None:
                with st.spinner("üîÆ Prediciendo..."):
                    # Calcular alpha seg√∫n configuraci√≥n
                    # Usar datos raw si existen
                    data_for_alpha = st.session_state.get('data_raw', None)
                    
                    if data_for_alpha is not None:
                        # Tenemos datos raw, calcular alpha din√°micamente
                        alpha = calculate_alpha(data_for_alpha, horizon=horizon, use_spread=use_spread)
                        alpha_calculated = True
                    else:
                        # Datos preprocesados, usar alpha te√≥rico por defecto
                        if use_spread:
                            alpha = 0.005  # 0.5% (spread t√≠pico de Bitcoin)
                        else:
                            alpha = 0.002  # 0.2% (volatilidad t√≠pica)
                        alpha_calculated = False
                        st.info(f"""
                        ‚ÑπÔ∏è Usando datos preprocesados. Alpha no calculado din√°micamente.
                        
                        Usando alpha te√≥rico por defecto:
                        - Normal: 0.2% (volatilidad t√≠pica)
                        - Spread: 0.5% (spread t√≠pico)
                        
                        Para c√°lculo din√°mico de alpha, usa datos crudos (CSV/NPY sin procesar).
                        """)
                    
                    # Guardar configuraci√≥n en session state
                    st.session_state['horizon'] = horizon
                    st.session_state['use_spread'] = use_spread
                    st.session_state['alpha'] = alpha
                    st.session_state['alpha_type'] = threshold_name
                    st.session_state['alpha_calculated'] = alpha_calculated
                    
                    logits, probs, pred = run_prediction(model, data)
                
                if pred is not None:
                    st.session_state['pred_result'] = {
                        'logits': logits,
                        'probs': probs,
                        'pred': pred
                    }
                    st.success("‚úÖ Predicci√≥n completada!")
                    st.balloons()
                    st.rerun()
    
    # TAB 4: Resultados
    with tab4:
        st.header("üìà Resultados")
        
        if 'pred_result' not in st.session_state:
            st.warning("‚ö†Ô∏è Ejecuta la predicci√≥n primero")
            return
        
        result = st.session_state['pred_result']
        logits = result['logits']
        probs = result['probs']
        pred = result['pred']
        
        label = CLASSES[pred]
        confidence = probs[pred]
        color = COLORS[pred]
        
        # Resultado principal
        st.markdown(f"""
        <div style="text-align: center; padding: 40px; background: linear-gradient(135deg, {color}22, {color}44); border-radius: 15px;">
            <h1 style="font-size: 70px; margin: 0;">{label.split()[1]}</h1>
            <h2 style="margin: 10px 0;">{label.split()[0]}</h2>
            <h3 style="color: {color}; margin: 0;">Confianza: {confidence:.1%}</h3>
        </div>
        """, unsafe_allow_html=True)
        
        st.divider()
        
        # Configuraci√≥n de la predicci√≥n
        if 'alpha' in st.session_state:
            alpha = st.session_state['alpha']
            alpha_type = st.session_state.get('alpha_type', 'Normal')
            horizon = st.session_state.get('horizon', 10)
            alpha_calculated = st.session_state.get('alpha_calculated', True)
            
            if alpha_calculated:
                alpha_label = f"**Alpha calculado:** {alpha:.4f} ({alpha*100:.2f}%)"
                alpha_note = "Calculado din√°micamente desde datos crudos"
            else:
                alpha_label = f"**Alpha te√≥rico:** {alpha:.4f} ({alpha*100:.2f}%)"
                alpha_note = "Valor por defecto (datos preprocesados)"
            
            st.info(f"""
            **Configuraci√≥n de la predicci√≥n:**
            - **Horizonte:** {horizon} timesteps
            - **Tipo de umbral:** {alpha_type}
            - {alpha_label}
            - *{alpha_note}*
            
            Los cambios de precio menores a ¬±{alpha*100:.2f}% se consideran **STATIONARY**.
            """)
        
        st.divider()
        
        # M√©tricas (Orden correcto: 0=UP, 1=STATIONARY, 2=DOWN)
        c1, c2, c3 = st.columns(3)
        c1.metric("üìà UP", f"{probs[0]:.1%}", f"Logit: {logits[0]:.2f}")
        c2.metric("‚û°Ô∏è STATIONARY", f"{probs[1]:.1%}", f"Logit: {logits[1]:.2f}")
        c3.metric("üìâ DOWN", f"{probs[2]:.1%}", f"Logit: {logits[2]:.2f}")
        
        st.divider()
        
        # Gr√°fico
        st.subheader("üìä Distribuci√≥n de Probabilidades")
        st.plotly_chart(plot_probs_bar(probs), use_container_width=True)
        
        # Interpretaci√≥n
        st.subheader("üí° Interpretaci√≥n")
        
        if confidence > 0.90:
            nivel = "**MUY ALTA** :green[‚≠ê‚≠ê‚≠ê]"
        elif confidence > 0.75:
            nivel = "**ALTA** :blue[‚≠ê‚≠ê]"
        elif confidence > 0.60:
            nivel = "**MODERADA** :orange[‚≠ê]"
        else:
            nivel = "**BAJA**"
        
        st.markdown(f"""
        Confianza {nivel} ({confidence:.1%})
        
        ‚Üí El precio tendr√° tendencia **{label.split()[0]}** en los pr√≥ximos **10 timesteps**.
        """)
        
        with st.expander("üî¨ Detalles T√©cnicos"):
            st.code(f"""
Shape entrada: {data.shape}
Mean: {data.mean():.4f}
Std: {data.std():.4f}

Logits:
  DOWN:       {logits[0]:>8.4f}
  STATIONARY: {logits[1]:>8.4f}
  UP:         {logits[2]:>8.4f}

Probabilidades (post-softmax):
  DOWN:       {probs[0]:>7.1%}
  STATIONARY: {probs[1]:>7.1%}
  UP:         {probs[2]:>7.1%}

Predicci√≥n: {label.split()[0]} (clase {pred})
            """)

if __name__ == "__main__":
    main()
