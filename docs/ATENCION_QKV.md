# Mecanismo de AtenciÃ³n del Modelo TLOB
## Queries (Q), Keys (K) y Values (V)

---

## ğŸ“š **Ãndice**

1. [IntroducciÃ³n](#introducciÃ³n)
2. [Arquitectura del Modelo TLOB](#arquitectura-del-modelo-tlob)
3. [Mecanismo de AtenciÃ³n](#mecanismo-de-atenciÃ³n)
4. [GeneraciÃ³n de Q, K, V](#generaciÃ³n-de-q-k-v)
5. [CÃ¡lculo de AtenciÃ³n](#cÃ¡lculo-de-atenciÃ³n)
6. [ImplementaciÃ³n en CÃ³digo](#implementaciÃ³n-en-cÃ³digo)
7. [Ejemplo PrÃ¡ctico](#ejemplo-prÃ¡ctico)
8. [Referencias](#referencias)

---

## ğŸ¯ **IntroducciÃ³n**

El modelo **TLOB (Transformer for Limit Order Book)** utiliza una arquitectura Transformer para predecir movimientos de precios en mercados financieros basÃ¡ndose en datos de Limit Order Book (LOB).

El componente clave del Transformer es el **mecanismo de atenciÃ³n multi-cabeza (Multi-Head Attention)**, que permite al modelo identificar relaciones importantes entre diferentes timesteps y features del LOB.

---

## ğŸ—ï¸ **Arquitectura del Modelo TLOB**

El modelo TLOB implementa una arquitectura Transformer dual que procesa los datos del LOB en dos dimensiones:

```
Input: (batch, seq_length=128, features=40)
         â†“
    BiN Normalization
         â†“
    Linear Embedding â†’ hidden_dim
         â†“
    Positional Encoding
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Transformer Layers (Ã—N)      â”‚
â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Feature Attention         â”‚ â”‚  â† AtenciÃ³n entre features
â”‚  â”‚ (temporal Ã— hidden_dim)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚             â†“                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Temporal Attention        â”‚ â”‚  â† AtenciÃ³n entre timesteps
â”‚  â”‚ (seq_length Ã— features)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Final MLP Layers
         â†“
Output: (batch, 3)  â†’ [DOWN, STATIONARY, UP]
```

---

## ğŸ” **Mecanismo de AtenciÃ³n**

### Â¿QuÃ© es la AtenciÃ³n?

La atenciÃ³n permite al modelo **enfocarse en diferentes partes de la entrada** al hacer predicciones. En el contexto del LOB:

- **AtenciÃ³n Temporal**: Â¿QuÃ© timesteps del pasado son mÃ¡s relevantes?
- **AtenciÃ³n Feature**: Â¿QuÃ© niveles de precios/volÃºmenes son mÃ¡s importantes?

### Componentes Clave

El mecanismo de atenciÃ³n se basa en tres matrices:

1. **Q (Queries)**: "Â¿QuÃ© estoy buscando?"
2. **K (Keys)**: "Â¿QuÃ© informaciÃ³n tengo disponible?"
3. **V (Values)**: "Â¿CuÃ¡l es el contenido real de esa informaciÃ³n?"

---

## âš™ï¸ **GeneraciÃ³n de Q, K, V**

### ImplementaciÃ³n en TLOB

```python
class ComputeQKV(nn.Module):
    def __init__(self, hidden_dim: int, num_heads: int):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # Tres proyecciones lineales independientes
        self.q = nn.Linear(hidden_dim, hidden_dim * num_heads)
        self.k = nn.Linear(hidden_dim, hidden_dim * num_heads)
        self.v = nn.Linear(hidden_dim, hidden_dim * num_heads)
        
    def forward(self, x):
        # x shape: (batch, seq_len, hidden_dim)
        q = self.q(x)  # (batch, seq_len, hidden_dim * num_heads)
        k = self.k(x)  # (batch, seq_len, hidden_dim * num_heads)
        v = self.v(x)  # (batch, seq_len, hidden_dim * num_heads)
        return q, k, v
```

### Proceso Paso a Paso

#### 1ï¸âƒ£ **Input Embeddings**

```
Input LOB: (batch=32, seq_len=128, features=40)
    â†“ BiN Normalization
    â†“ Linear Embedding
Embedded: (batch=32, seq_len=128, hidden_dim=256)
```

#### 2ï¸âƒ£ **Proyecciones Lineales**

Cada embedding pasa por **tres transformaciones lineales independientes**:

```python
# Para cada posiciÃ³n temporal t en la secuencia:
Q[t] = W_q @ x[t] + b_q  # ProyecciÃ³n Query
K[t] = W_k @ x[t] + b_k  # ProyecciÃ³n Key
V[t] = W_v @ x[t] + b_v  # ProyecciÃ³n Value
```

Donde:
- `W_q`, `W_k`, `W_v` son matrices de pesos aprendibles
- `x[t]` es el embedding en el timestep t

#### 3ï¸âƒ£ **Multi-Head Attention**

Las proyecciones se dividen en mÃºltiples "cabezas" (heads=8):

```
Q: (batch=32, seq_len=128, hidden_dim*num_heads=256*8)
    â†“ Reshape
Q: (batch=32, num_heads=8, seq_len=128, head_dim=256)

K: (batch=32, num_heads=8, seq_len=128, head_dim=256)
V: (batch=32, num_heads=8, seq_len=128, head_dim=256)
```

---

## ğŸ§® **CÃ¡lculo de AtenciÃ³n**

### FÃ³rmula de Scaled Dot-Product Attention

```
Attention(Q, K, V) = softmax(Q @ K^T / âˆšd_k) @ V
```

### Paso a Paso

#### 1ï¸âƒ£ **Scores de AtenciÃ³n**

```python
# Producto punto entre Queries y Keys
scores = Q @ K.transpose(-2, -1)  # (batch, heads, seq_len, seq_len)

# Ejemplo: Para el timestep t=100
# scores[100, :] indica quÃ© tanto debe "atender" t=100 a todos los otros timesteps
```

#### 2ï¸âƒ£ **Scaling**

```python
d_k = hidden_dim  # 256
scores = scores / math.sqrt(d_k)  # NormalizaciÃ³n para estabilidad numÃ©rica
```

Â¿Por quÃ© dividir por âˆšd_k?
- Evita que los valores sean demasiado grandes
- Previene que el softmax sature

#### 3ï¸âƒ£ **Softmax (Pesos de AtenciÃ³n)**

```python
attention_weights = softmax(scores, dim=-1)  # (batch, heads, seq_len, seq_len)
```

Los pesos suman 1 para cada timestep:
```
âˆ‘ attention_weights[t, :] = 1.0
```

InterpretaciÃ³n:
- `attention_weights[100, 50] = 0.3` â†’ El timestep 100 presta 30% de atenciÃ³n al timestep 50

#### 4ï¸âƒ£ **Weighted Sum de Values**

```python
output = attention_weights @ V  # (batch, heads, seq_len, head_dim)
```

Cada timestep obtiene una **combinaciÃ³n ponderada** de todos los Values:

```
output[t] = Î£ (attention_weights[t, s] * V[s])
            s=0...seq_len
```

---

## ğŸ’» **ImplementaciÃ³n en CÃ³digo**

### Clase TransformerLayer Completa

```python
class TransformerLayer(nn.Module):
    def __init__(self, hidden_dim: int, num_heads: int, final_dim: int):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # Layer Normalization
        self.norm = nn.LayerNorm(hidden_dim)
        
        # MÃ³dulo de Q, K, V
        self.qkv = ComputeQKV(hidden_dim, num_heads)
        
        # Multi-Head Attention de PyTorch
        self.attention = nn.MultiheadAttention(
            hidden_dim * num_heads, 
            num_heads, 
            batch_first=True
        )
        
        # MLP Feed-Forward
        self.mlp = MLP(hidden_dim, hidden_dim*4, final_dim)
        
        # ProyecciÃ³n final
        self.w0 = nn.Linear(hidden_dim * num_heads, hidden_dim)
        
    def forward(self, x):
        # Residual connection
        res = x
        
        # 1. Generar Q, K, V
        q, k, v = self.qkv(x)
        
        # 2. Aplicar atenciÃ³n multi-cabeza
        x, att = self.attention(q, k, v, 
                                average_attn_weights=False, 
                                need_weights=True)
        
        # 3. ProyecciÃ³n lineal
        x = self.w0(x)
        
        # 4. Residual connection
        x = x + res
        
        # 5. Layer Normalization
        x = self.norm(x)
        
        # 6. Feed-Forward MLP
        x = self.mlp(x)
        
        # 7. Segunda residual connection (si dimensiones coinciden)
        if x.shape[-1] == res.shape[-1]:
            x = x + res
            
        return x, att  # Retorna output y pesos de atenciÃ³n
```

---

## ğŸ“ **Ejemplo PrÃ¡ctico**

### Datos de Entrada: Limit Order Book

```python
import numpy as np
import torch

# Datos del LOB (simplificado)
batch_size = 1
seq_len = 128  # 128 timesteps (32 segundos @ 250ms)
features = 40  # 10 niveles Ã— 4 (ASK_P, ASK_V, BID_P, BID_V)

lob_data = torch.randn(batch_size, seq_len, features)
```

### Dimensiones en Cada Paso

```python
# 1. Input
input_shape = (1, 128, 40)

# 2. DespuÃ©s de BiN Normalization
normalized_shape = (1, 128, 40)

# 3. DespuÃ©s de Embedding
hidden_dim = 256
embedded_shape = (1, 128, 256)

# 4. Q, K, V
num_heads = 8
qkv_shape = (1, 128, 256*8)  # (1, 128, 2048)

# 5. Reshape para Multi-Head
# (batch, seq_len, hidden*heads) â†’ (batch, heads, seq_len, head_dim)
q_multihead = (1, 8, 128, 256)
k_multihead = (1, 8, 128, 256)
v_multihead = (1, 8, 128, 256)

# 6. Attention Scores
scores = (1, 8, 128, 128)  # Cada timestep atiende a todos los timesteps

# 7. Attention Weights (despuÃ©s de softmax)
attention_weights = (1, 8, 128, 128)

# 8. Output (weighted sum de Values)
attention_output = (1, 8, 128, 256)

# 9. Concatenar cabezas
output = (1, 128, 2048)

# 10. ProyecciÃ³n final
final_output = (1, 128, 256)
```

### VisualizaciÃ³n de Pesos de AtenciÃ³n

```python
import matplotlib.pyplot as plt
import seaborn as sns

# att shape: (1, num_heads, seq_len, seq_len)
att_weights = att[0, 0, :, :].detach().cpu().numpy()  # Primera cabeza

plt.figure(figsize=(10, 8))
sns.heatmap(att_weights, cmap='viridis', cbar=True)
plt.title('Attention Weights - Head 0')
plt.xlabel('Key Position (timestep)')
plt.ylabel('Query Position (timestep)')
plt.show()
```

**InterpretaciÃ³n del heatmap:**
- **Filas (Query)**: Timestep que estÃ¡ "preguntando"
- **Columnas (Key)**: Timesteps disponibles para "responder"
- **Color brillante**: Alta atenciÃ³n â†’ Ese timestep es importante
- **Color oscuro**: Baja atenciÃ³n â†’ Ese timestep es menos relevante

---

## ğŸ”¬ **Innovaciones del Modelo TLOB**

### 1. **AtenciÃ³n Dual (Temporal + Feature)**

A diferencia de Transformers tradicionales, TLOB aplica atenciÃ³n en **dos dimensiones**:

```python
# Capa 1: AtenciÃ³n sobre FEATURES (quÃ© niveles del LOB son importantes)
feature_att, att1 = transformer_layer1(x)  # (batch, seq_len, hidden_dim)

# Capa 2: AtenciÃ³n sobre TIEMPO (quÃ© timesteps son importantes)
temporal_att, att2 = transformer_layer2(x.transpose(1, 2))  # (batch, hidden_dim, seq_len)
```

### 2. **BiN Normalization**

NormalizaciÃ³n especializada para datos de series temporales financieras:

```python
class BiN(nn.Module):
    """Batch-Instance Normalization para LOB"""
    def forward(self, x):
        # Normaliza por batch Y por instancia
        batch_norm = (x - x.mean(dim=0)) / x.std(dim=0)
        instance_norm = (x - x.mean(dim=(1,2))) / x.std(dim=(1,2))
        return 0.5 * batch_norm + 0.5 * instance_norm
```

### 3. **Positional Encoding**

Codifica la posiciÃ³n temporal en el LOB:

```python
if is_sin_emb:
    # Sinusoidal (como en "Attention is All You Need")
    pos_encoder = sinusoidal_positional_embedding(seq_size, hidden_dim)
else:
    # Aprendible
    pos_encoder = nn.Parameter(torch.randn(1, seq_size, hidden_dim))
```

---

## ğŸ“Š **InterpretaciÃ³n de los Pesos de AtenciÃ³n**

### Â¿QuÃ© nos dicen los pesos de atenciÃ³n?

Los pesos de atenciÃ³n revelan **quÃ© informaciÃ³n del pasado usa el modelo** para hacer predicciones:

#### Ejemplo 1: AtenciÃ³n Temporal

```
PredicciÃ³n en t=128:
- attention_weights[127, 126] = 0.15  â† Presta 15% atenciÃ³n al timestep inmediato anterior
- attention_weights[127, 100] = 0.08  â† 8% a timestep 28 segundos atrÃ¡s
- attention_weights[127, 50]  = 0.03  â† 3% a timestep lejano
```

**InterpretaciÃ³n**: El modelo considera principalmente los timesteps recientes, pero tambiÃ©n mira eventos importantes del pasado.

#### Ejemplo 2: AtenciÃ³n Feature

```
Para predecir el movimiento:
- Alta atenciÃ³n a sell1/buy1 (primer nivel del LOB) â†’ Spread es importante
- Media atenciÃ³n a sell2-sell5 â†’ Profundidad del mercado relevante
- Baja atenciÃ³n a sell8-sell10 â†’ Niveles lejanos menos importantes
```

---

## ğŸ¯ **Ventajas del Mecanismo de AtenciÃ³n**

### 1. **Dependencias de Largo Alcance**

```python
# RNN/LSTM: InformaciÃ³n se "olvida" con el tiempo
# Transformer: Puede atender a CUALQUIER timestep del pasado
attention_weights[127, 0] = 0.02  # Puede mirar el primer timestep directamente
```

### 2. **ParalelizaciÃ³n**

```python
# RNN: Procesa secuencialmente (t=1 â†’ t=2 â†’ t=3 â†’ ...)
# Transformer: Procesa TODOS los timesteps en paralelo
Q, K, V = compute_qkv(all_timesteps)  # Una sola operaciÃ³n matricial
```

### 3. **Interpretabilidad**

```python
# Los pesos de atenciÃ³n son interpretables
# Podemos visualizar QUÃ‰ mira el modelo para hacer predicciones
plot_attention_heatmap(attention_weights)
```

---

## ğŸ“š **Referencias**

1. **ArtÃ­culo Original**: [TLOB: Transformer for Limit Order Books](https://arxiv.org/abs/2110.00551)
2. **Attention is All You Need**: [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)
3. **Repositorio Original**: [GitHub - TLOB](https://github.com/SiddharthKarnam/TLOB)
4. **Multi-Head Attention**: [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)

---

## ğŸ”— **Resumen Visual del Flujo Completo**

```
LOB Data (128 timesteps Ã— 40 features)
         â†“
    BiN Normalization
         â†“
    Embedding (hidden_dim=256)
         â†“
    Add Positional Encoding
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ComputeQKV                 â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ Q = Linear(x)       â”‚   â”‚  â† Queries
    â”‚  â”‚ K = Linear(x)       â”‚   â”‚  â† Keys
    â”‚  â”‚ V = Linear(x)       â”‚   â”‚  â† Values
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Multi-Head Attention       â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ Scores = Q @ K^T    â”‚   â”‚
    â”‚  â”‚ Weights = Softmax   â”‚   â”‚
    â”‚  â”‚ Output = Weights@V  â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Residual Connection + LayerNorm
         â†“
    Feed-Forward MLP
         â†“
    PredicciÃ³n: [DOWN, STATIONARY, UP]
```

---

**Ãšltima actualizaciÃ³n**: Noviembre 2025  
**Autor**: Proyecto Final - AnÃ¡lisis de Series Temporales con Transformers


