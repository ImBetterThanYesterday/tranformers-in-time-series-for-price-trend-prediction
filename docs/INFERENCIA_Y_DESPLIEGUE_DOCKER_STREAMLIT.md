# Inferencia y Despliegue: Gu√≠a Completa

## Tabla de Contenidos

1. [Introducci√≥n](#1-introducci√≥n)
2. [Flujo Completo de Inferencia](#2-flujo-completo-de-inferencia)
3. [Preprocesamiento de Datos](#3-preprocesamiento-de-datos)
4. [Despliegue con Docker](#4-despliegue-con-docker)
5. [Aplicaci√≥n Streamlit](#5-aplicaci√≥n-streamlit)
6. [Casos de Uso](#6-casos-de-uso)
7. [Troubleshooting](#7-troubleshooting)
8. [Referencias](#8-referencias)

---

## 1. Introducci√≥n

Este documento explica el proceso completo de inferencia del modelo TLOB, desde la carga de datos hasta la visualizaci√≥n de predicciones en Streamlit. El despliegue se realiza mediante Docker para garantizar portabilidad y facilidad de uso.

### Componentes del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Datos LOB      ‚îÇ --> ‚îÇ  Modelo TLOB    ‚îÇ --> ‚îÇ  Streamlit UI   ‚îÇ
‚îÇ  (CSV/NPY)      ‚îÇ     ‚îÇ  (PyTorch)      ‚îÇ     ‚îÇ  (Visualizaci√≥n)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. Flujo Completo de Inferencia

### 2.1 Diagrama de Flujo Detallado

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          INICIO: Cargar Archivo                            ‚îÇ
‚îÇ                       (CSV o NPY desde Streamlit)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Detectar Formato ‚îÇ
                    ‚îÇ y Tipo de Datos  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                             ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ  Archivo .CSV     ‚îÇ      ‚îÇ   Archivo .NPY      ‚îÇ
     ‚îÇ  (siempre crudo)  ‚îÇ      ‚îÇ  (crudo o normal.)  ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                             ‚îÇ
              ‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                    ‚îÇ Detectar si est√°‚îÇ
              ‚îÇ                    ‚îÇ  Normalizado    ‚îÇ
              ‚îÇ                    ‚îÇ  (mean ‚âà 0?)    ‚îÇ
              ‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                             ‚îÇ
              ‚îÇ                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                 ‚îÇ                       ‚îÇ
              ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ          ‚îÇ  Ya Normal  ‚îÇ      ‚îÇ   Datos Crudos  ‚îÇ
              ‚îÇ          ‚îÇ  (mean‚âà0,   ‚îÇ      ‚îÇ  (mean>>1000)   ‚îÇ
              ‚îÇ          ‚îÇ   std‚âà1)    ‚îÇ      ‚îÇ                 ‚îÇ
              ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                 ‚îÇ                      ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  ¬øNecesita            ‚îÇ
                    ‚îÇ  Normalizaci√≥n?       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ                       ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ   S√ç (Crudo)    ‚îÇ     ‚îÇ   NO (Normalizado)‚îÇ
           ‚îÇ                 ‚îÇ     ‚îÇ                   ‚îÇ
           ‚îÇ Aplicar Z-score ‚îÇ     ‚îÇ Usar tal cual     ‚îÇ
           ‚îÇ Normalization   ‚îÇ     ‚îÇ                   ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ                      ‚îÇ
                    ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Datos Normalizados        ‚îÇ
         ‚îÇ Shape: (128, 40)          ‚îÇ
         ‚îÇ Mean ‚âà 0, Std ‚âà 1        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Convertir a Tensor        ‚îÇ
         ‚îÇ tensor.float().to(DEVICE) ‚îÇ
         ‚îÇ Shape: (1, 128, 40)       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Cargar Modelo TLOB        ‚îÇ
         ‚îÇ seg√∫n horizonte:          ‚îÇ
         ‚îÇ - 10, 20, 50, 100 steps   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Inferencia (Forward Pass) ‚îÇ
         ‚îÇ with torch.no_grad():     ‚îÇ
         ‚îÇ   logits = model(tensor)  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Aplicar Softmax           ‚îÇ
         ‚îÇ probs = softmax(logits)   ‚îÇ
         ‚îÇ Shape: (1, 3)             ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ ‚ö†Ô∏è INVERTIR ORDEN         ‚îÇ
         ‚îÇ probs_inv = [probs[2],    ‚îÇ
         ‚îÇ              probs[1],    ‚îÇ
         ‚îÇ              probs[0]]    ‚îÇ
         ‚îÇ (ver docs/FIX_ORDEN...)   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Obtener Predicci√≥n        ‚îÇ
         ‚îÇ pred = argmax(probs_inv)  ‚îÇ
         ‚îÇ 0=UP, 1=STAT, 2=DOWN      ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Visualizar en Streamlit   ‚îÇ
         ‚îÇ - Gr√°ficos                ‚îÇ
         ‚îÇ - M√©tricas                ‚îÇ
         ‚îÇ - Heatmaps                ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2.2 Detecci√≥n Autom√°tica de Normalizaci√≥n

El c√≥digo en `app.py` detecta autom√°ticamente si los datos est√°n normalizados:

```python
def is_data_normalized(data):
    """
    Detecta si los datos ya est√°n normalizados
    
    Heur√≠stica:
    - Si mean >> 100: Datos crudos (precios BTC en USDT)
    - Si mean ‚âà 0 y std ‚âà 1: Ya normalizados (z-scores)
    """
    mean = np.abs(data.mean())
    std = data.std()
    
    if mean > 100:
        return False, "raw"  # Datos crudos
    elif mean < 1 and 0.5 < std < 2:
        return True, "normalized"  # Ya normalizado
    else:
        return None, "unknown"  # No estamos seguros
```

**Ejemplo con Datos Reales**:

```python
# Datos crudos BTC
raw_data = np.array([[42150.5, 0.524, 42148.2, 0.631, ...]])  # Precios en USDT
mean = 21075.0  # >> 100
is_normalized(raw_data)  # --> (False, "raw")

# Datos normalizados
norm_data = np.array([[0.523, 0.145, -0.412, 0.223, ...]])  # Z-scores
mean = 0.0001  # ‚âà 0
is_normalized(norm_data)  # --> (True, "normalized")
```

---

## 3. Preprocesamiento de Datos

### 3.1 Z-Score Normalization

**F√≥rmula**:

$$
x_{norm} = \frac{x - \mu}{\sigma}
$$

Donde:
- $x$: Valor original
- $\mu$: Media
- $\sigma$: Desviaci√≥n est√°ndar

**Implementaci√≥n en `app.py`**:

```python
def normalize_raw_data(data):
    """
    Normaliza datos crudos separando precios y vol√∫menes
    
    Input: (128, 40) - valores crudos
    Output: (128, 40) - z-scores
    """
    df = pd.DataFrame(data)
    
    # Columnas pares = precios, impares = vol√∫menes
    mean_prices = df.iloc[:, 0::2].stack().mean()
    std_prices = df.iloc[:, 0::2].stack().std()
    mean_volumes = df.iloc[:, 1::2].stack().mean()
    std_volumes = df.iloc[:, 1::2].stack().std()
    
    # Normalizar por tipo
    for col in df.columns[0::2]:  # Precios
        df[col] = (df[col] - mean_prices) / std_prices
    
    for col in df.columns[1::2]:  # Vol√∫menes
        df[col] = (df[col] - mean_volumes) / std_volumes
    
    return df.values
```

### 3.2 Ejemplo Num√©rico Completo

#### Entrada (Datos Crudos):

```
Timestep  ASK_P1     ASK_V1   BID_P1     BID_V1   ...
0         42150.50   0.524    42148.20   0.631    ...
1         42151.20   0.489    42148.50   0.702    ...
...       ...        ...      ...        ...      ...
127       42155.80   0.512    42152.10   0.598    ...
```

**Estad√≠sticas**:
- `mean_prices = 42152.35 USDT`
- `std_prices = 2.45 USDT`
- `mean_volumes = 0.567 BTC`
- `std_volumes = 0.089 BTC`

#### Salida (Datos Normalizados):

```
Timestep  ASK_P1   ASK_V1   BID_P1   BID_V1   ...
0         -0.755   -0.483   -1.691   0.719    ...
1         -0.469   -0.876   -1.569   1.517    ...
...       ...      ...      ...      ...      ...
127        1.410   -0.618   -0.102   0.348    ...
```

**Estad√≠sticas**:
- `mean_normalized = 0.0001` ‚úì
- `std_normalized = 0.998` ‚úì

### 3.3 Validaci√≥n de Shape

```python
# Verificar shape correcto
assert data.shape == (128, 40), f"Shape incorrecto: {data.shape}"

# 128 timesteps √ó 40 features
# Features: [ASK_P1, ASK_V1, BID_P1, BID_V1, ..., ASK_P10, ASK_V10, BID_P10, BID_V10]
```

---

## 4. Despliegue con Docker

### 4.1 M√©todo Principal: Docker Compose

**Opci√≥n Recomendada - Un Solo Comando**:

```bash
# 1. Clonar repositorio
git clone https://github.com/tu-usuario/tlob-prediction.git
cd tlob-prediction

# 2. Levantar aplicaci√≥n con un solo comando
docker-compose up -d

# ‚úÖ Listo! La app estar√° disponible en http://localhost:8501
```

**Verificar que est√° corriendo**:

```bash
# Ver logs en tiempo real
docker-compose logs -f

# Verificar estado
docker-compose ps

# Output esperado:
# NAME              STATUS        PORTS
# tlob-streamlit    Up 2 minutes  0.0.0.0:8501->8501/tcp
```

### 4.2 M√©todo Alternativo: Docker Build Manual

Si prefieres m√°s control:

```bash
# 1. Construir imagen
docker build -t tlob-app:latest .

# 2. Ejecutar contenedor
docker run -d \
  --name tlob-container \
  -p 8501:8501 \
  -v $(pwd)/src/data:/app/src/data:ro \
  tlob-app:latest

# 3. Verificar logs
docker logs -f tlob-container
```

### 4.3 Dockerfile Explicado

```dockerfile
# Imagen base ligera
FROM python:3.12-slim

# Directorio de trabajo
WORKDIR /app

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y gcc g++ git

# Copiar requirements e instalar
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo fuente
COPY . .

# Configurar PYTHONPATH para imports
ENV PYTHONPATH=/app:${PYTHONPATH}

# Exponer puerto de Streamlit
EXPOSE 8501

# Comando de inicio
CMD ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

**Puntos Clave**:
1. `PYTHONPATH=/app`: Permite que Python encuentre el m√≥dulo `src`
2. `--server.address=0.0.0.0`: Permite acceso desde fuera del contenedor
3. `--no-cache-dir`: Reduce tama√±o de imagen
4. `python:3.12-slim`: Imagen base ligera (~150MB vs ~1GB de python:3.12)

### 4.4 Docker Compose Explicado

```yaml
services:
  tlob-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: tlob-streamlit
    ports:
      - "8501:8501"  # Puerto host:contenedor
    volumes:
      - ./src/data:/app/src/data:ro  # Solo lectura
    environment:
      - STREAMLIT_SERVER_PORT=8501
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
    restart: unless-stopped  # Reiniciar autom√°ticamente
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - tlob-network

networks:
  tlob-network:
    driver: bridge
```

**Ventajas de Docker Compose**:
- ‚úÖ Configuraci√≥n declarativa (YAML)
- ‚úÖ Un solo comando para levantar todo
- ‚úÖ Gesti√≥n autom√°tica de redes
- ‚úÖ Healthchecks integrados
- ‚úÖ F√°cil escalar a m√∫ltiples servicios

### 4.5 Comandos √ötiles de Docker

```bash
# Detener aplicaci√≥n
docker-compose down

# Reconstruir imagen (despu√©s de cambios en c√≥digo)
docker-compose up -d --build

# Ver logs de errores
docker-compose logs --tail=50

# Entrar al contenedor para debugging
docker-compose exec tlob-app /bin/bash

# Limpiar todo (contenedores, im√°genes, vol√∫menes)
docker-compose down -v
docker system prune -af
```

---

## 5. Aplicaci√≥n Streamlit

### 5.1 Arquitectura de la App

```
app.py
‚îú‚îÄ‚îÄ Sidebar (Configuraci√≥n)
‚îÇ   ‚îú‚îÄ‚îÄ Cargar Datos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Selector de Fuente (Preprocesados / Crudos)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Lista de Ejemplos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ File Uploader
‚îÇ   ‚îî‚îÄ‚îÄ Info del Modelo
‚îÇ
‚îú‚îÄ‚îÄ Tab 1: üìä Datos
‚îÇ   ‚îú‚îÄ‚îÄ M√©tricas (Shape, Mean, Std)
‚îÇ   ‚îú‚îÄ‚îÄ Heatmap (128√ó40)
‚îÇ   ‚îú‚îÄ‚îÄ Series Temporales
‚îÇ   ‚îî‚îÄ‚îÄ Comparaci√≥n Raw vs Normalized
‚îÇ
‚îú‚îÄ‚îÄ Tab 2: üîç An√°lisis
‚îÇ   ‚îú‚îÄ‚îÄ 40 Histogramas
‚îÇ   ‚îî‚îÄ‚îÄ Tabla de Estad√≠sticas
‚îÇ
‚îú‚îÄ‚îÄ Tab 3: üéØ Predicci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ Selector de Horizonte (10/20/50/100)
‚îÇ   ‚îú‚îÄ‚îÄ Selector de Umbral (Normal/Spread)
‚îÇ   ‚îú‚îÄ‚îÄ Bot√≥n "Ejecutar Predicci√≥n"
‚îÇ   ‚îî‚îÄ‚îÄ Info sobre Etiquetado
‚îÇ
‚îî‚îÄ‚îÄ Tab 4: üìà Resultados
    ‚îú‚îÄ‚îÄ Predicci√≥n Principal (grande)
    ‚îú‚îÄ‚îÄ Gr√°fico de Probabilidades
    ‚îú‚îÄ‚îÄ M√©tricas por Clase
    ‚îî‚îÄ‚îÄ Detalles T√©cnicos
```

### 5.2 Screenshots Principales

#### Screenshot 1: Carga de Datos

**[PLACEHOLDER: Screenshot mostrando el sidebar con selector de fuente y bot√≥n de carga]**

**Descripci√≥n**: 
- Radio buttons para elegir fuente (Preprocesados / Crudos)
- Dropdown con lista de archivos disponibles
- Bot√≥n "Cargar" para confirmar
- File uploader para archivos personalizados

---

#### Screenshot 2: Visualizaci√≥n de Datos

**[PLACEHOLDER: Screenshot del Tab "Datos" mostrando heatmap y series temporales]**

**Descripci√≥n**:
- Heatmap interactivo de 128√ó40 (Plotly)
- Series temporales de ASK/BID prices y volumes
- Comparaci√≥n lado a lado de datos raw vs normalizados (si aplicable)

---

#### Screenshot 3: Configuraci√≥n de Predicci√≥n

**[PLACEHOLDER: Screenshot del Tab "Predicci√≥n" con selectores de horizonte y umbral]**

**Descripci√≥n**:
- Selector de horizonte (10, 20, 50, 100 timesteps)
- Radio buttons para tipo de umbral (Normal / Spread)
- Info box explicando el etiquetado
- Bot√≥n grande "Ejecutar Predicci√≥n"

---

#### Screenshot 4: Resultado de Predicci√≥n

**[PLACEHOLDER: Screenshot del Tab "Resultados" mostrando predicci√≥n UP con 85% de confianza]**

**Descripci√≥n**:
- Card grande central con predicci√≥n y emoji
- Color de fondo seg√∫n clase (verde/azul/rojo)
- Porcentaje de confianza
- M√©tricas de probabilidades por clase

---

#### Screenshot 5: Gr√°fico de Probabilidades

**[PLACEHOLDER: Screenshot del gr√°fico de barras con probabilidades de las 3 clases]**

**Descripci√≥n**:
- Barra chart interactivo (Plotly)
- 3 barras: UP (verde), STATIONARY (azul), DOWN (rojo)
- Valores en porcentaje
- Etiquetas claras

---

### 5.3 Gesti√≥n de Estado con Session State

```python
# Variables clave en st.session_state
st.session_state = {
    # Datos
    'data': np.array,              # (128, 40) normalizado
    'data_raw': np.array,          # (128, 40) crudo (opcional)
    'filename': str,               # Nombre del archivo
    'source': str,                 # "Preprocesados" o "Crudos"
    
    # Modelos (cach√©)
    'tlob_model_h10': TLOB,        # Modelo horizonte 10
    'tlob_model_h20': TLOB,        # Modelo horizonte 20
    'tlob_model_h50': TLOB,        # Modelo horizonte 50
    'tlob_model_h100': TLOB,       # Modelo horizonte 100
    'current_horizon': int,        # Horizonte actual
    
    # Resultados de predicci√≥n
    'pred_result': dict,           # {'logits': [...], 'probs': [...], 'pred': int}
    'horizon': int,                # Horizonte usado
    'use_spread': bool,            # Tipo de umbral
    'alpha': float,                # Alpha calculado
    'alpha_type': str,             # "Normal" o "Spread"
}
```

**Ventajas**:
- No recargar modelos en cada interacci√≥n
- Mantener datos cargados entre tabs
- Preservar resultados de predicciones
- UX fluida sin p√©rdida de estado

### 5.4 Flujo de Usuario T√≠pico

```
1. Usuario abre http://localhost:8501
   ‚Üì
2. Ve pantalla inicial con info y bot√≥n de carga
   ‚Üì
3. Selecciona fuente de datos en sidebar
   ‚Üì
4. Elige archivo y hace click en "Cargar"
   ‚Üì
5. Si es crudo: app detecta y normaliza autom√°ticamente
   ‚Üì
6. TAB 1: Visualiza heatmap y series temporales
   ‚Üì
7. TAB 2: Explora distribuciones de features
   ‚Üì
8. TAB 3: Configura horizonte (ej: 10) y umbral (Normal)
   ‚Üì
9. Click en "Ejecutar Predicci√≥n"
   ‚Üì
10. App carga modelo de horizonte 10 (o usa cach√©)
    ‚Üì
11. Ejecuta forward pass del modelo
    ‚Üì
12. Invierte orden de softmax (cr√≠tico!)
    ‚Üì
13. TAB 4: Muestra resultado con gr√°ficos y m√©tricas
    ‚Üì
14. Usuario puede probar otro horizonte o cargar otro archivo
```

---

## 6. Casos de Uso

### 6.1 Caso 1: Predicci√≥n con Datos Preprocesados

```python
# Datos ya est√°n normalizados (de src/data/BTC/individual_examples/)
# Shape: (128, 40), mean‚âà0, std‚âà1

# 1. Cargar en Streamlit
uploaded_file = "example_1.npy"

# 2. No requiere normalizaci√≥n
is_normalized(data)  # --> True

# 3. Directo a inferencia
logits, probs, pred = run_prediction(model, data)

# 4. Mostrar resultado
st.success(f"Predicci√≥n: {CLASSES[pred]}")
```

### 6.2 Caso 2: Predicci√≥n con Datos Crudos (CSV)

```python
# Datos crudos desde Binance (precios en USDT, vol√∫menes en BTC)
# Shape: (128, 40), mean>>1000

# 1. Cargar CSV
uploaded_file = "raw_example_1.csv"
data_raw = pd.read_csv(uploaded_file).values  # (128, 40)

# 2. Detectar que necesita normalizaci√≥n
is_normalized(data_raw)  # --> False

# 3. Aplicar Z-score
data_normalized = normalize_raw_data(data_raw)

# 4. Inferencia
logits, probs, pred = run_prediction(model, data_normalized)

# 5. Mostrar resultado
st.success(f"Predicci√≥n: {CLASSES[pred]}")
```

### 6.3 Caso 3: Comparar M√∫ltiples Horizontes

```python
# Probar diferentes horizontes de predicci√≥n

horizontes = [10, 20, 50, 100]
resultados = {}

for h in horizontes:
    # Cargar modelo correspondiente
    model = get_model(horizon=h)
    
    # Ejecutar predicci√≥n
    logits, probs, pred = run_prediction(model, data)
    
    # Guardar
    resultados[h] = {
        'pred': CLASSES[pred],
        'confianza': probs[pred]
    }

# Visualizar comparaci√≥n
st.table(pd.DataFrame(resultados).T)
```

**Ejemplo de Output**:

```
Horizonte | Predicci√≥n  | Confianza
----------|-------------|----------
10        | UP üìà      | 85.2%
20        | UP üìà      | 78.4%
50        | STATIONARY | 65.1%
100       | DOWN üìâ    | 72.3%
```

**Interpretaci√≥n**: A corto plazo (10-20 steps) predice UP, pero a largo plazo (100 steps) predice DOWN.

---

## 7. Troubleshooting

### 7.1 Problemas Comunes

#### Error: "No module named 'src'"

**Causa**: PYTHONPATH no incluye el directorio ra√≠z.

**Soluci√≥n**:

```bash
# En Docker
ENV PYTHONPATH=/app:${PYTHONPATH}

# En local
export PYTHONPATH=/path/to/tlob-prediction:$PYTHONPATH
```

#### Error: "Shape incorrecto: (128, 42)"

**Causa**: CSV incluye columnas extra (timestamp, datetime).

**Soluci√≥n**:

```python
# Eliminar columnas no necesarias
if 'timestamp' in df.columns:
    df = df.drop(columns=['timestamp', 'datetime'])

data = df.values  # Ahora shape=(128, 40) ‚úì
```

#### Error: "AttributeError: 'UploadedFile' object has no attribute 'suffix'"

**Causa**: Streamlit UploadedFile no tiene `.suffix` directamente.

**Soluci√≥n**:

```python
# Usar Path(uploaded_file.name).suffix
from pathlib import Path

file_extension = Path(uploaded_file.name).suffix  # '.csv' o '.npy'
```

#### Error: Predicci√≥n siempre STATIONARY

**Causa**: Modelo no est√° cargando correctamente o datos no est√°n normalizados.

**Verificar**:

```python
# 1. Verificar mean y std de datos
print(f"Mean: {data.mean()}, Std: {data.std()}")
# Esperado: Mean ‚âà 0, Std ‚âà 1

# 2. Verificar que modelo carg√≥
print(f"Par√°metros: {sum(p.numel() for p in model.parameters())}")
# Esperado: ~1,100,000

# 3. Verificar que est√° en modo eval
print(model.training)  # False esperado
```

### 7.2 Performance Issues

#### Streamlit Lento

**Optimizaciones**:

```python
# 1. Usar @st.cache_data para cargar modelo
@st.cache_resource
def load_model(horizon):
    return get_model(horizon)

# 2. Cachear preprocesamiento
@st.cache_data
def preprocess_data(file_bytes):
    return normalize_raw_data(np.load(file_bytes))

# 3. Usar session_state para resultados
if 'pred_result' not in st.session_state:
    st.session_state['pred_result'] = run_prediction(model, data)
```

#### Docker Usa Mucha Memoria

**Optimizar Dockerfile**:

```dockerfile
# Usar multi-stage build
FROM python:3.12-slim as builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

FROM python:3.12-slim
WORKDIR /app
COPY --from=builder /root/.local /root/.local
COPY . .
ENV PATH=/root/.local/bin:$PATH
```

**Resultado**: Imagen ~400MB en lugar de ~1.2GB

---

## 8. Referencias

### Documentaci√≥n Relacionada

1. **Mecanismo de Atenci√≥n**: [`docs/MECANISMO_ATENCION_QKV.md`](MECANISMO_ATENCION_QKV.md)
   - Teor√≠a matem√°tica de Q, K, V
   - Ejemplo paso a paso con datos TLOB

2. **Innovaciones del Modelo**: [`docs/INNOVACIONES_TLOB.md`](INNOVACIONES_TLOB.md)
   - Dual Attention
   - BiN Normalization
   - Comparaci√≥n vs otros modelos

3. **Arquitectura Completa**: [`docs/ARQUITECTURA_COMPLETA.md`](ARQUITECTURA_COMPLETA.md)
   - Estructura de 4 pares de Transformers
   - Dimensiones en cada capa
   - Forward pass detallado

4. **README Principal**: [`README.md`](../README.md)
   - Introducci√≥n al proyecto
   - Instalaci√≥n r√°pida
   - Resultados y m√©tricas

### Paper y Repositorio Original

```bibtex
@article{berti2025tlob,
  title={TLOB: A Novel Transformer Model with Dual Attention for Stock Price Trend Prediction with Limit Order Book Data},
  author={Berti, Leonardo and Kasneci, Gjergji},
  journal={arXiv preprint arXiv:2502.15757},
  year={2025}
}
```

- **Paper**: https://arxiv.org/pdf/2502.15757
- **Repositorio Original**: https://github.com/LeonardoBerti00/TLOB

### Herramientas Utilizadas

- **PyTorch**: https://pytorch.org/
- **Streamlit**: https://streamlit.io/
- **Docker**: https://www.docker.com/
- **Plotly**: https://plotly.com/python/
- **NumPy**: https://numpy.org/

---

**√öltima actualizaci√≥n**: Noviembre 2025  
**Versi√≥n**: 1.0.0

---

## Ap√©ndice A: Checklist de Despliegue

Usa este checklist para verificar que todo est√° configurado correctamente:

### Pre-Despliegue

- [ ] Docker y Docker Compose instalados
- [ ] Repositorio clonado
- [ ] Checkpoints del modelo presentes en `src/data/checkpoints/TLOB/`
- [ ] Datos de ejemplo presentes en `src/data/BTC/`

### Despliegue

- [ ] `docker-compose up -d` ejecutado sin errores
- [ ] `docker-compose ps` muestra contenedor "Up"
- [ ] http://localhost:8501 accesible en navegador
- [ ] Sidebar muestra lista de ejemplos
- [ ] Se puede cargar un archivo sin errores

### Post-Despliegue

- [ ] Predicci√≥n funciona con datos preprocesados
- [ ] Predicci√≥n funciona con datos crudos (CSV)
- [ ] Se pueden cambiar horizontes (10, 20, 50, 100)
- [ ] Gr√°ficos se visualizan correctamente
- [ ] No hay errores en `docker-compose logs`

### Troubleshooting

- [ ] Si falla: revisar logs con `docker-compose logs -f`
- [ ] Si no carga modelo: verificar PYTHONPATH en Dockerfile
- [ ] Si shape incorrecto: verificar que datos son (128, 40)
- [ ] Si siempre predice igual: verificar normalizaci√≥n de datos

---

**Fin del Documento**

