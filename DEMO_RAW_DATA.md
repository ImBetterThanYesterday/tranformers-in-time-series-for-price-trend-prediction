# üöÄ Demo: Procesamiento e Inferencia con Datos Crudos de BTC

Este documento muestra c√≥mo procesar datos **crudos** (raw) del CSV original de Kaggle y realizar inferencia con el modelo TLOB entrenado.

## üìä Dataset Original

- **Fuente**: [Kaggle - Bitcoin Perpetual LOB Data](https://www.kaggle.com/datasets/siavashraz/bitcoin-perpetualbtcusdtp-limit-order-book-data)
- **Exchange**: Binance (BTCUSDT.P)
- **Per√≠odo**: 9-20 Enero 2023 (12 d√≠as consecutivos)
- **Frecuencia**: 250ms (4 muestras por segundo)
- **Total de filas**: 3,730,870

### Estructura del CSV Original

| Columna | Descripci√≥n |
|---------|-------------|
| 0 | Index |
| 1 | Timestamp (microsegundos UTC) |
| 2 | Datetime |
| 3-12 | BID Price Levels 1-10 |
| 13-22 | BID Volume Levels 1-10 |
| 23-32 | ASK Price Levels 1-10 |
| 33-42 | ASK Volume Levels 1-10 |

**Total**: 43 columnas (1 index + 42 columnas de datos)

---

## üîÑ Pipeline Completo

### Paso 1: Procesar Muestras Crudas

El script `process_raw_btc_samples.py` realiza:

1. **Carga** el CSV original
2. **Reordena** las columnas al formato esperado por el modelo:
   - ASK Price, ASK Vol, BID Price, BID Vol (alternando por cada nivel)
3. **Extrae** N muestras aleatorias de 128 timesteps consecutivos
4. **Normaliza** con Z-score (usando estad√≠sticas de la propia muestra)
5. **Guarda** archivos `.npy` listos para inferencia

```bash
# Procesar 10 muestras del CSV original
python3 process_raw_btc_samples.py --num_samples 10

# Opciones adicionales
python3 process_raw_btc_samples.py \
    --num_samples 20 \
    --seq_size 128 \
    --csv_path data/BTC/original_source/1-09-1-20.csv \
    --output_dir data/BTC/raw_samples
```

**Salida**:
```
data/BTC/raw_samples/
‚îú‚îÄ‚îÄ raw_sample_1.npy         # Muestra individual (128, 40)
‚îú‚îÄ‚îÄ raw_sample_2.npy
‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ raw_sample_10.npy
‚îú‚îÄ‚îÄ raw_samples_batch.npy    # Todas las muestras (10, 128, 40)
‚îú‚îÄ‚îÄ metadata.json            # Metadatos y estad√≠sticas
‚îî‚îÄ‚îÄ README.md                # Documentaci√≥n
```

---

### Paso 2: Inferencia Individual

Usa `inference_single_file.py` para predecir sobre una muestra:

```bash
python3 inference_single_file.py data/BTC/raw_samples/raw_sample_1.npy
```

**Ejemplo de salida**:

```
================================================================================
                           üéØ RESULTADO DE INFERENCIA                            
================================================================================

üìÇ Archivo: data/BTC/raw_samples/raw_sample_1.npy
üìä Shape: (128, 40) (timesteps √ó features)

üì• Estad√≠sticas de Entrada:
   Mean:  -0.0000
   Std:    0.9998
   Min:   -1.0011
   Max:    1.0001

üé≤ Probabilidades:
   üìâ DOWN:         0.82%
   ‚û°Ô∏è  STATIONARY:  96.12%
   üìà UP:           3.06%

********************************************************************************
                     üéØ PREDICCI√ìN: ‚û°Ô∏è STATIONARY (clase 1)                      
                              üí™ CONFIANZA:  96.12%                              
********************************************************************************
```

---

### Paso 3: Inferencia en Batch

Procesa todas las muestras de una vez:

```bash
# Opci√≥n 1: Archivo batch
python3 inference_pytorch.py \
    --examples_path data/BTC/raw_samples/raw_samples_batch.npy

# Opci√≥n 2: Iterar sobre individuales
for i in {1..10}; do
    python3 inference_single_file.py data/BTC/raw_samples/raw_sample_${i}.npy
done
```

---

## üß™ Ejemplo Completo: Del CSV Raw a la Predicci√≥n

### 1. Verificar el CSV original

```bash
# Ver primeras l√≠neas
head -5 data/BTC/original_source/1-09-1-20.csv

# Contar filas
wc -l data/BTC/original_source/1-09-1-20.csv
```

### 2. Procesar muestras

```bash
python3 process_raw_btc_samples.py --num_samples 10
```

### 3. Inspeccionar muestras generadas

```python
import numpy as np

# Cargar una muestra
sample = np.load('data/BTC/raw_samples/raw_sample_1.npy')
print(f"Shape: {sample.shape}")  # (128, 40)
print(f"Mean: {sample.mean():.4f}")
print(f"Std: {sample.std():.4f}")

# Ver primeras 5 filas, 10 columnas
print(sample[:5, :10])
```

### 4. Realizar inferencia

```bash
python3 inference_single_file.py data/BTC/raw_samples/raw_sample_1.npy
```

### 5. Ver resultados guardados

```bash
# Resultado num√©rico
cat data/BTC/raw_samples/raw_sample_1_result.txt

# Resultado como array
python3 -c "
import numpy as np
result = np.load('data/BTC/raw_samples/raw_sample_1_result.npy')
print(f'Logits: {result}')
"
```

---

## üìê Detalles T√©cnicos del Preprocesamiento

### Reordenamiento de Columnas

**CSV Original**:
```
[Index, Timestamp, Datetime, BID_P1-P10, BID_V1-V10, ASK_P1-P10, ASK_V1-V10]
```

**Formato del Modelo** (alternando por nivel):
```
[Timestamp, ASK_P1, ASK_V1, BID_P1, BID_V1, ASK_P2, ASK_V2, BID_P2, BID_V2, ...]
```

### Normalizaci√≥n Z-Score

Para cada feature:
```python
normalized_value = (value - mean) / std
```

- **Precios** (columnas pares): Usan `mean_prices` y `std_prices`
- **Vol√∫menes** (columnas impares): Usan `mean_size` y `std_size`

### Ventana de Inferencia

- **Tama√±o**: 128 timesteps consecutivos
- **Duraci√≥n temporal**: ~32 segundos (128 √ó 250ms)
- **Features**: 40 (10 niveles del LOB √ó 4 tipos de datos)

---

## üéØ Casos de Uso

### Caso 1: Testing con Nuevos Per√≠odos

Si obtienes datos de otro per√≠odo temporal (e.g., Febrero 2023):

```bash
# Procesar el nuevo CSV
python3 process_raw_btc_samples.py \
    --csv_path data/BTC/original_source/2-01-2-15.csv \
    --num_samples 20 \
    --output_dir data/BTC/raw_samples_feb
```

### Caso 2: Inferencia en Tiempo Real (Simulado)

Extrae ventanas consecutivas en lugar de aleatorias:

```python
# Modificar extract_samples() para ventanas consecutivas
start_indices = range(0, max_start_idx, seq_size)  # Sin overlapping
```

### Caso 3: Evaluar Diferentes Horizontes

El modelo actual predice a **horizon=10** (10 timesteps adelante = 2.5 segundos).

Para evaluar otros horizontes, necesitas:
1. Entrenar modelos con diferentes `h` (20, 50, 100)
2. Usar los checkpoints correspondientes en inferencia

---

## üìä Comparaci√≥n: Datos Preprocesados vs Raw

| Aspecto | `train.npy` (Preprocesado) | Raw CSV |
|---------|----------------------------|---------|
| **Fuente** | Ya procesado y guardado | CSV original de Kaggle |
| **Normalizaci√≥n** | Estad√≠sticas del training set | Estad√≠sticas propias |
| **Labels** | Incluye 4 columnas de labels | Solo LOB (40 features) |
| **Formato** | (N, 44) | (N, 43) ‚Üí (ventana, 40) |
| **Uso** | Training y evaluaci√≥n | Inferencia en nuevos datos |

**Ventaja de usar raw**: Puedes procesar **cualquier** per√≠odo temporal nuevo sin depender de los datos preprocesados.

---

## üîß Troubleshooting

### Error: "Index out of bounds"

**Causa**: El CSV tiene menos filas que `num_samples √ó seq_size`.

**Soluci√≥n**:
```bash
# Reducir n√∫mero de muestras
python3 process_raw_btc_samples.py --num_samples 5
```

### Error: "Columns mismatch"

**Causa**: El CSV tiene formato diferente.

**Soluci√≥n**: Verificar que el CSV tenga exactamente 43 columnas (1 index + 42 datos).

### Advertencia: "Normalization stats differ"

**Causa**: Las estad√≠sticas de normalizaci√≥n de la muestra raw difieren del training set.

**Impacto**: Puede afectar ligeramente la precisi√≥n del modelo. Para m√°xima precisi√≥n, usa las estad√≠sticas del training set.

---

## üìö Referencias

- **Dataset Original**: https://www.kaggle.com/datasets/siavashraz/bitcoin-perpetualbtcusdtp-limit-order-book-data
- **Art√≠culo TLOB**: Temporal Limit Order Book for Price Trend Prediction
- **C√≥digo de Preprocesamiento**: `preprocessing/btc.py`
- **Script de Procesamiento Raw**: `process_raw_btc_samples.py`
- **Script de Inferencia**: `inference_single_file.py`

---

## ‚úÖ Checklist para Nuevos Datos

- [ ] Descargar CSV de Kaggle o exchange
- [ ] Verificar estructura (43 columnas)
- [ ] Ejecutar `process_raw_btc_samples.py`
- [ ] Verificar archivos `.npy` generados
- [ ] Probar inferencia con una muestra
- [ ] Analizar resultados y m√©tricas

---

**√öltima actualizaci√≥n**: $(date)
**Generado autom√°ticamente por el pipeline de procesamiento**

