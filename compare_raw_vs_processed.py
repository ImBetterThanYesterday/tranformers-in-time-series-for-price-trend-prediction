#!/usr/bin/env python3
"""
Script de Comparaci√≥n: Datos Raw vs Preprocesados
=================================================
Compara las muestras crudas procesadas con los datos del training set.

Uso:
    python3 compare_raw_vs_processed.py
"""

import numpy as np
import json
from pathlib import Path

print("=" * 80)
print("COMPARACI√ìN: DATOS RAW VS PREPROCESADOS")
print("=" * 80)

# ============================================================================
# 1. CARGAR DATOS
# ============================================================================
print("\n1. CARGANDO DATOS")
print("-" * 80)

# Datos preprocesados (training set)
train_data = np.load('data/BTC/train.npy')
print(f"‚úì Training set: {train_data.shape}")
print(f"  - Features: {train_data.shape[1]} (40 LOB + 4 labels)")

# Datos raw procesados
raw_samples = np.load('data/BTC/raw_samples/raw_samples_batch.npy')
print(f"‚úì Raw samples: {raw_samples.shape}")
print(f"  - Samples: {raw_samples.shape[0]}")
print(f"  - Timesteps: {raw_samples.shape[1]}")
print(f"  - Features: {raw_samples.shape[2]}")

# Metadata
with open('data/BTC/raw_samples/metadata.json', 'r') as f:
    metadata = json.load(f)
print(f"‚úì Metadata cargada")

# ============================================================================
# 2. ESTAD√çSTICAS GENERALES
# ============================================================================
print("\n2. ESTAD√çSTICAS GENERALES")
print("-" * 80)

# Training set (solo LOB features, sin labels)
train_lob = train_data[:, :40]

print("\nüìä Training Set (preprocesado):")
print(f"   Shape: {train_lob.shape}")
print(f"   Mean: {train_lob.mean():.6f}")
print(f"   Std: {train_lob.std():.6f}")
print(f"   Min: {train_lob.min():.6f}")
print(f"   Max: {train_lob.max():.6f}")

print("\nüìä Raw Samples (reci√©n procesados):")
print(f"   Shape: {raw_samples.shape}")
print(f"   Mean: {raw_samples.mean():.6f}")
print(f"   Std: {raw_samples.std():.6f}")
print(f"   Min: {raw_samples.min():.6f}")
print(f"   Max: {raw_samples.max():.6f}")

# ============================================================================
# 3. ESTAD√çSTICAS DE NORMALIZACI√ìN
# ============================================================================
print("\n3. ESTAD√çSTICAS DE NORMALIZACI√ìN")
print("-" * 80)

norm_stats = metadata['normalization_stats']

print("\nüîß Estad√≠sticas usadas en Raw Samples:")
print(f"   Mean Prices: {norm_stats['mean_prices']:.4f}")
print(f"   Std Prices:  {norm_stats['std_prices']:.4f}")
print(f"   Mean Volumes: {norm_stats['mean_size']:.4f}")
print(f"   Std Volumes:  {norm_stats['std_size']:.4f}")

# Calcular estad√≠sticas del training set
price_cols = list(range(0, 40, 2))
volume_cols = list(range(1, 40, 2))

train_mean_prices = train_lob[:, price_cols].mean()
train_std_prices = train_lob[:, price_cols].std()
train_mean_volumes = train_lob[:, volume_cols].mean()
train_std_volumes = train_lob[:, volume_cols].std()

print("\nüîß Estad√≠sticas del Training Set:")
print(f"   Mean Prices: {train_mean_prices:.4f}")
print(f"   Std Prices:  {train_std_prices:.4f}")
print(f"   Mean Volumes: {train_mean_volumes:.4f}")
print(f"   Std Volumes:  {train_std_volumes:.4f}")

print("\nüìè Diferencias:")
diff_mean_prices = abs(norm_stats['mean_prices'] - train_mean_prices)
diff_std_prices = abs(norm_stats['std_prices'] - train_std_prices)
diff_mean_volumes = abs(norm_stats['mean_size'] - train_mean_volumes)
diff_std_volumes = abs(norm_stats['std_size'] - train_std_volumes)

print(f"   Œî Mean Prices: {diff_mean_prices:.4f} "
      f"({diff_mean_prices/train_mean_prices*100:.2f}%)")
print(f"   Œî Std Prices:  {diff_std_prices:.4f} "
      f"({diff_std_prices/train_std_prices*100:.2f}%)")
print(f"   Œî Mean Volumes: {diff_mean_volumes:.4f} "
      f"({diff_mean_volumes/train_mean_volumes*100:.2f}%)")
print(f"   Œî Std Volumes:  {diff_std_volumes:.4f} "
      f"({diff_std_volumes/train_std_volumes*100:.2f}%)")

# ============================================================================
# 4. COMPARACI√ìN POR FEATURE
# ============================================================================
print("\n4. COMPARACI√ìN POR FEATURE (Primeras 10)")
print("-" * 80)

print(f"\n{'Feature':<10} {'Train Mean':<12} {'Train Std':<12} {'Raw Mean':<12} {'Raw Std':<12}")
print("-" * 58)

for i in range(10):
    train_feat = train_lob[:, i]
    raw_feat = raw_samples[:, :, i].flatten()
    
    print(f"{i:<10} {train_feat.mean():>11.4f} {train_feat.std():>11.4f} "
          f"{raw_feat.mean():>11.4f} {raw_feat.std():>11.4f}")

# ============================================================================
# 5. AN√ÅLISIS DE DISTRIBUCI√ìN
# ============================================================================
print("\n5. AN√ÅLISIS DE DISTRIBUCI√ìN")
print("-" * 80)

# Percentiles
print("\nüìä Percentiles (todas las features combinadas):")
print(f"{'Dataset':<20} {'p5':<10} {'p25':<10} {'p50':<10} {'p75':<10} {'p95':<10}")
print("-" * 70)

train_flat = train_lob.flatten()
raw_flat = raw_samples.flatten()

train_percentiles = [np.percentile(train_flat, p) for p in [5, 25, 50, 75, 95]]
raw_percentiles = [np.percentile(raw_flat, p) for p in [5, 25, 50, 75, 95]]

print(f"{'Training Set':<20} {train_percentiles[0]:<10.4f} {train_percentiles[1]:<10.4f} "
      f"{train_percentiles[2]:<10.4f} {train_percentiles[3]:<10.4f} {train_percentiles[4]:<10.4f}")
print(f"{'Raw Samples':<20} {raw_percentiles[0]:<10.4f} {raw_percentiles[1]:<10.4f} "
      f"{raw_percentiles[2]:<10.4f} {raw_percentiles[3]:<10.4f} {raw_percentiles[4]:<10.4f}")

# ============================================================================
# 6. COMPATIBILIDAD PARA INFERENCIA
# ============================================================================
print("\n6. COMPATIBILIDAD PARA INFERENCIA")
print("-" * 80)

print("\n‚úÖ Verificaciones:")

# 1. Shape correcta
shape_ok = raw_samples.shape[1:] == (128, 40)
print(f"   {'‚úÖ' if shape_ok else '‚ùå'} Shape correcta (128, 40): {shape_ok}")

# 2. No hay NaN o Inf
no_nan = not np.isnan(raw_samples).any()
no_inf = not np.isinf(raw_samples).any()
print(f"   {'‚úÖ' if no_nan else '‚ùå'} Sin NaN: {no_nan}")
print(f"   {'‚úÖ' if no_inf else '‚ùå'} Sin Inf: {no_inf}")

# 3. Rango razonable
range_ok = (raw_samples.min() > -5) and (raw_samples.max() < 5)
print(f"   {'‚úÖ' if range_ok else '‚ö†Ô∏è'} Rango razonable (-5, 5): {range_ok}")
print(f"      Actual: ({raw_samples.min():.4f}, {raw_samples.max():.4f})")

# 4. Distribuci√≥n similar
distribution_similar = abs(raw_samples.mean()) < 0.5 and abs(raw_samples.std() - 1.0) < 0.5
print(f"   {'‚úÖ' if distribution_similar else '‚ö†Ô∏è'} Distribuci√≥n similar a Z-score: {distribution_similar}")
print(f"      Mean: {raw_samples.mean():.4f} (esperado ~0.0)")
print(f"      Std:  {raw_samples.std():.4f} (esperado ~1.0)")

# ============================================================================
# 7. RESUMEN Y RECOMENDACIONES
# ============================================================================
print("\n7. RESUMEN Y RECOMENDACIONES")
print("-" * 80)

all_checks_pass = shape_ok and no_nan and no_inf and range_ok

if all_checks_pass:
    print("\n‚úÖ TODAS LAS VERIFICACIONES PASARON")
    print("\n   Las muestras raw est√°n correctamente procesadas y son compatibles")
    print("   con el modelo TLOB para inferencia.")
    
    if not distribution_similar:
        print("\n‚ö†Ô∏è  NOTA: La distribuci√≥n de las raw samples difiere ligeramente")
        print("   del training set. Esto es NORMAL y esperado porque:")
        print("   1. Las raw samples usan sus propias estad√≠sticas de normalizaci√≥n")
        print("   2. Representan un per√≠odo temporal diferente")
        print("   3. El modelo deber√≠a poder generalizarlas correctamente")
else:
    print("\n‚ùå ALGUNAS VERIFICACIONES FALLARON")
    print("\n   Revisa los problemas indicados arriba antes de usar las")
    print("   muestras para inferencia.")

print("\nüìä DIFERENCIAS CLAVE:")
print("   ‚Ä¢ Training Set: Normalizado con estad√≠sticas del per√≠odo completo")
print("   ‚Ä¢ Raw Samples: Normalizado con estad√≠sticas de ventanas individuales")
print("   ‚Ä¢ Impacto: M√≠nimo para inferencia, el modelo es robusto a estas variaciones")

print("\nüí° RECOMENDACIONES:")
print("   1. Para m√°xima precisi√≥n, usa estad√≠sticas del training set")
print("   2. Para nuevos per√≠odos, las estad√≠sticas propias son adecuadas")
print("   3. Siempre verifica que los datos no tengan NaN/Inf")
print("   4. El rango de valores debe estar entre -3 y +3 (t√≠pico de Z-score)")

print("\n" + "=" * 80)
print("‚úì COMPARACI√ìN COMPLETADA")
print("=" * 80)

